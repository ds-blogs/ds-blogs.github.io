{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"MLOps/","title":"Machine Learning Operations (MLOps)","text":""},{"location":"MLOps/#machine-learning-life-cycle","title":"Machine Learning Life-Cycle","text":""},{"location":"MLOps/docker-guide/","title":"Docker Guide","text":""},{"location":"MLOps/docker-guide/#what-is-docker-image-docker-container","title":"What is Docker Image &amp; Docker Container?","text":"<p>Docker Image is an executable package of software that includes everything needed to run an application. This image informs how a container should instantiate, determining which software components will run and how. Docker Container is a virtual environment that bundles application code with all the dependencies required to run the application. The application runs quickly and reliably from one computing environment to another. One Docker Container only can hold one Docker Image.</p> <p>You can find the images from Docker Hub.</p>"},{"location":"MLOps/docker-guide/#reference-of-docker-some-important-commands","title":"Reference of Docker | Some Important Commands","text":""},{"location":"MLOps/docker-guide/#check-the-version","title":"Check the version:","text":"<p>There are 2 commands that can help to check the docker version <pre><code>docker version\n</code></pre> or <pre><code>docker -v\n</code></pre></p>"},{"location":"MLOps/docker-guide/#docker-pull","title":"<code>docker pull</code>:","text":"<p>Pull an image or a repository from a registry/docker hub. Like to use postgres, you have to pull that image first by the below command:</p> <pre><code>docker pull postgres\n</code></pre> <p>When you don't mention the tag, this command will pull the latest image. If you want to pull postgres image version 14, then the command will be</p> <pre><code>docker pull postgres:14\n</code></pre> <p>Note: <code>:14</code> is called <code>tag</code>.</p>"},{"location":"MLOps/docker-guide/#docker-image","title":"<code>docker image</code>:","text":"<ul> <li>Using this command, you can see list of images are there in your local machine. This command gives you some information about the docker images. <pre><code>docker image ls\n</code></pre></li> </ul>"},{"location":"MLOps/docker-guide/#docker-ps","title":"<code>docker ps</code>:","text":"<p>Same as <code>docker image</code> command, this command helps to work with containers. For example, to list down all the containers of your local machine, this below command is used. <pre><code>docker ps\n</code></pre></p>"},{"location":"MLOps/docker-guide/#docker-run","title":"<code>docker run</code>:","text":"<p>One of the most used command in docker family. It helps to run the image that you have pulled or have created to your local machine. E.g.,</p> <p><pre><code>docker run postgres\n</code></pre> or <pre><code>docker run postgres:13.8\n</code></pre> The above command will run the <code>latest postgres image</code>. postgres is the image name. If you want to run a specific version of the image, you have to specify as so called <code>tag</code>.</p> <p>Important options: - <code>-it</code>: This instructs docker to allocate a pseudo.TTY connected to the container's stdin; creating an interactive bash shell in the container. - <code>-e ENVIRONMENT_VAR=VALUE</code>: Some images need environment variables which help to run the image properly. E.g., to run postgres properly, you have to set password by <code>POSTGRES_PASSWORD=mysecretpassword</code>. - <code>-d</code>: Detach mode. When you run the image, it keeps your terminal busy. You can enter or command any other command or have to keep open the terminal to run the image continuously. If you pass this option, then the terminal will be detached and you can work with it. - <code>--name some-name</code>: If you don't specify this option, docker comes up a name on its own and sometimes it is confusing. For that, you can name the container. You can't name same twice or without deleting the previous container. - <code>-p host_port:container_port</code>: Some docker image(s) you run together. But it is possible that the port is same and that can conflict to your application. For that, you can open a port (host_port) to your local machine that will communicate with the container_port. Very important option. To know which port the container is using, you can use <code>docker ps</code> command. This concept is known as <code>port mapping</code>. - - <code>--net network-name</code>: Connect the container to a network. This can help to connect other containers with this containers easily. Otherwise different networks can't connect each other.</p>"},{"location":"MLOps/docker-guide/#docker-stop-or-docker-container-stop","title":"<code>docker stop</code> or <code>docker container stop</code>:","text":"<p>Stop one or more running containers.</p> <pre><code>docker stop container_name\nor\ndocker stop container_id\nor\ndocker container stop container_name\nor\ndocker container stop container_id\n</code></pre>"},{"location":"MLOps/docker-guide/#docker-container-ls","title":"<code>docker container ls</code>:","text":"<p>An alternative of the command <code>docker ps</code> to list out all the active docker containers.</p> <pre><code>docker container ls\n</code></pre> <p>If you want all the containers whatever it is running or not, command this: <pre><code>docker container ls -a\n</code></pre></p>"},{"location":"MLOps/docker-guide/#docker-container-prune","title":"<code>docker container prune</code>:","text":"<p>Remove all the stopped containers from your local machine. This is a very dangerous command. You have to be very careful when you are using it. Though it does remove the stopped containers but does not remove the volumes associated with the containers.</p> <pre><code>docker container prune\n</code></pre>"},{"location":"MLOps/docker-guide/#docker-logs","title":"<code>docker logs</code>:","text":"<p>You can see the logs of the containers. It helps to get inside what is executing in containers.</p> <pre><code>docker logs container_name\nor\ndocker logs container_id\n</code></pre>"},{"location":"MLOps/docker-guide/#docker-example","title":"Docker Example:","text":""},{"location":"MLOps/docker-guide/#connect-docker-mongo-express-with-mongodb-docker-compose","title":"Connect docker mongo-express with mongodb | Docker Compose:","text":"<p>MongoDB is the database and the MongoDB-Express is the Web-based MongoDB admin interface, written with Node.js and express. So, we will start the container first of the MongoDB and then also start the MongoDB-Express container by connecting with the MongoDB network.</p> <p>Create a network that both containers will connect together. <pre><code>docker network create mongo-network\n</code></pre></p> <p>Start the MongoDB container: <pre><code>docker run -p 27017:27017 -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=password --name mongodb --net mongo-network -d mongo\n</code></pre></p> <p>Now start the MongoDB-Express container: <pre><code>docker run --network mongo-network -e ME_CONFIG_MONGODB_SERVER=mongodb -e ME_CONFIG_MONGODB_ADMINUSERNAME=admin -e ME_CONFIG_MONGODB_ADMINPASSWORD=password -p 8081:8081 -d --name mongo-express mongo-express\n</code></pre> Now the web app of mongodb-express container is running and you can visit by using the browser. The url is http://localhost:8081. You can visit it. If we stop the <code>mongodb</code> container that we have created, it will also stop the <code>mongo-express</code> container because <code>mongodb</code> container is the dependency of the <code>mongo-express</code> container.</p> <p>So, we have wrote lots of long commands to do this. This can cause issue if you have done a small spelling mistake. To solve this issue, <code>docker compose</code> come out. For this you can create a yml file. The file name can be anything. For our case, we can name it as docker-compose.yml.</p> <p><pre><code>version: '3'\nservices:\nmongodb:\nimage: mongo\nports:\n- \"27017:27017\"\nenvironment:\n- MONGO_INITDB_ROOT_USERNAME=admin\n- MONGO_INITDB_ROOT_PASSWORD=password\nvolumes:\n- mymongo-data:/data/db\nmongo-express:\nimage: mongo-express\nrestart: always\nports:\n- \"8081:8081\"\nenvironment:\n- ME_CONFIG_MONGODB_SERVER=mongodb\n- ME_CONFIG_MONGODB_ADMINUSERNAME=admin\n- ME_CONFIG_MONGODB_ADMINPASSWORD=password\nvolumes:\nmymongo-data:\ndriver: local\n</code></pre> After creating the compose yml file, you can run the file by the following command:</p> <pre><code>docker-compose -f docker-compose.yml up\n</code></pre> <p>The best thing with docker compose is that it will create a network by-default and will put all the containers into it. We don't have to specify separately in the docker compose file. Check out the <code>mongodb</code> container. One option is <code>volumes</code>. That is different with the lowers <code>volumes</code> settings. The value of<code>volumes</code> of <code>mongodb</code> container should be default value of the mongo image. That can be different with MySQL or any other image. For that, you have to look to the documentation of the images. By using the <code>volumes</code>, you can detach and attach the volume(s) with the container according to the requirements.</p> <p>To stop this compose, you can run the command to a another terminal. This will also remove the containers from your local machine automatically. You don't have to do it manually.</p> <pre><code>docker-compose -f docker-compose.yml down\n</code></pre>"},{"location":"MLOps/docker-guide/#flask-app-create-docker-image-push-image-to-docker-hub","title":"Flask APP | Create Docker Image | Push Image to Docker Hub:","text":"<p>To create an docker image, you have to create a file named <code>Dockerfile</code> inside your root project directory. According to the demo-flask app, add the below commands to the <code>Dockerfile</code>.</p> <pre><code># define the base image\nFROM python:3-alpine3.15\n# define the working directory\nWORKDIR /app\n# copy everything and move those to the directory\nCOPY . /app\n# install the required python packages using pip\nRUN pip install -r requirements.txt\n# expose the port in which this application will run.\n# This is defined to the python flask file.\nEXPOSE 3000\n# Now start the application, the entry point of your app\nCMD python ./index.py\n</code></pre> <p>Now to build the image of your flask app, you can run the below command. <code>sayanroy7</code> is the username of mine (to get your username, you have to create an account to docker hub). <code>flask-docker-demo</code> is the image of the image that you are giving. <code>:0.0.1.RELEASE</code> is the <code>flag</code> of this image.</p> <pre><code>docker build -t sayanroy7/flask-docker-demo:0.0.1.RELEASE .\n</code></pre> <p>After building the image, you can see it to your Docker Desktop app. Before pushing to the hub, check that is it working fine or not. For that, run the simple command:</p> <pre><code>docker container run -d -p 3000:3000 --name flask-docker-demo sayanroy7/flask-docker-demo:0.0.1.RELEASE\n</code></pre> <p>If you go to the browser and hit http://localhost:3000, you should get the result as</p> <pre><code>{\"message\":\"Hey there Python\"}\n</code></pre> <p>If the image is working fine, then push the image to the Docker Hub by the command. If you already signed in to your Docker Desktop, then the image will be uploaded smoothly otherwise it will prompt for the authentications.</p> <pre><code>docker push sayanroy7/flask-docker-demo:0.0.1.RELEASE </code></pre> <p>And in your Docker Hub, you can see the image under the Repositories tab!! </p>"},{"location":"MLOps/deployment/common-deployment-cases/","title":"Deployment Methods","text":""},{"location":"MLOps/deployment/common-deployment-cases/#common-deployment-cases","title":"Common Deployment Cases","text":""},{"location":"MLOps/deployment/common-deployment-cases/#new-product-or-capability","title":"New product or capability","text":"<p>One type of deployment is if you are offering a new product or capability that you had not previously offered. For example, if you're offering a speech recognition service that you have not offered before, in this case, a common design pattern is to start up a small amount of traffic and then gradually ramp it up.</p>"},{"location":"MLOps/deployment/common-deployment-cases/#automateassist-with-manual-task","title":"Automate/assist with manual task","text":"<p>A second common deployment use case is if there's something that's already being done by a person, but we would now like to use a learning algorithm to either automate or assist with that task. For example, if you have people in the factory inspecting smartphones scratches, but now you would like to use a learning algorithm to either assist or automate that task. The fact that people were previously doing this gives you a few more options for how you deploy. And you see shadow mode deployment takes advantage of this.</p>"},{"location":"MLOps/deployment/common-deployment-cases/#replace-previous-ml-system","title":"Replace previous ML system","text":"<p>a third common deployment case is if you've already been doing this task with a previous implementation of a machine learning system, but you now want to replace it with hopefully an even better one. In these cases, two recurring themes you see are that you often want a gradual ramp up with monitoring. In other words, rather than sending tons of traffic to a maybe not fully proven learning algorithm, you may send it only a small amount of traffic and monitor it and then ramp up the percentage or amount of traffic. And the second idea you see a few times is rollback. Meaning that if for some reason the algorithm isn't working, it's nice if you can revert back to the previous system if indeed there was an earlier system.</p>"},{"location":"MLOps/deployment/common-deployment-cases/#different-deployment-techniques","title":"Different Deployment Techniques:","text":""},{"location":"MLOps/deployment/common-deployment-cases/#shadow-mode-deployment","title":"Shadow Mode Deployment","text":"<p>you've had human inspectors inspect smartphones for defects for scratches. And you would now like to automate some of this work with a learning algorithm. When you have people initially doing a task, one common deployment pattern is to use shadow mode deployment. And what that means is that you will start by having a machine learning algorithm shadow the human inspector and running parallel with the human inspector. During this initial phase, the learning algorithms output is not used for any decision in the factory. So whatever the learning algorithm says, we're going to go the human judgment for now.</p> <p> </p> <p>So let's say for this smartphone the human says it's fine, no defect. The learning algorithm says it's fine. Maybe for this example of a big stretch down the middle, person says it's not okay and the learning algorithm agrees. And maybe for this example with a smaller stretch, maybe the person says this is not okay, but the learning algorithm makes a mistake and actually thinks this is okay. The purpose of a shadow mode deployment is that allows you to gather data of how the learning algorithm is performing and how that compares to the human judgment. And by something the output you can then verify if the learning algorithm's predictions are accurate and therefore use that to decide whether or not to maybe allow the learning algorithm to make some real decisions in the future. So when you already have some system that is making good decisions and that system can be human inspectors or even an older implementation of a learning algorithm. Using a shadow mode deployment can be a very effective way to let you verify the performance of a learning algorithm before letting them make any real decisions.</p>"},{"location":"MLOps/deployment/common-deployment-cases/#canary-deployment","title":"Canary Deployment","text":"<p>When you are ready to let a learning algorithm start making real decisions, a common deployment pattern is to use a canary deployment. And in a canary deployments you would roll out to a small fraction, maybe 5%, maybe even less of traffic initially and start let the algorithm making real decisions. But by running this on only a small percentage of the traffic, hopefully, if the algorithm makes any mistakes it will affect only a small fraction of the traffic. And this gives you more of an opportunity to monitor the system and ramp up the percentage of traffic it gets only gradually and only when you have greater confidence in this performance. The phrase canary deployment is a reference to the English idiom or the English phrase canary in a coal mine, which refers to how coal miners used to use canaries to spot if there's a gas leak. But with canary the deployment, hopefully this allows you to spot problems early on before there are maybe overly large consequences to a factory or other context in which you're deploying your learning algorithm.</p>"},{"location":"MLOps/deployment/common-deployment-cases/#blue-green-deployment","title":"Blue Green Deployment","text":"<p> <pre><code>flowchart LR\n    %% Colors %%\n    classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff\n    classDef green fill:#16b552,stroke:#000,stroke-width:2px,color:#fff\n\n    P(Phone images) ====&gt; R(Route)\n    R ====&gt; O(Old/Blue version):::blue\n    R ====&gt; N(New/Green version):::green\n</code></pre> </p> <p>Say you have a system, a camera software for collecting phone pictures in your factory. These phone images are sent to a piece of software that takes these images and routes them into some visual inspection system. In the terminology of a blue green deployments, the old version of your software is called the blue version and the new version, the Learning algorithm you just implemented is called the green version. In a blue green deployment, what you do is have the router send images to the old or the blue version and have that make decisions. And then when you want to switch over to the new version, what you would do is have the router stop sending images to the old one and suddenly switch over to the new version. So the way the blue green deployment is implemented is you would have an old prediction service may be running on some sort of service. You will then spin up a new prediction service, the green version, and you would have the router suddenly switch the traffic over from the old one to the new one. </p> <p>The advantage of a blue green deployment is that there's an easy way to enable rollback. If something goes wrong, you can just very quickly have the router go back reconfigure their router to send traffic back to the old or the blue version, assuming that you kept your blue version of the prediction service running. In a typical implementation of a blue green deployment, people think of switching over the traffic 100% all at the same time. But of course you can also use a more gradual version where you slowly send traffic over.</p>"},{"location":"MLOps/deployment/common-deployment-cases/#degrees-of-automation","title":"Degrees of Automation","text":"<p>As you can imagine, whether use shadow mode, canary mode, blue green, or some of the deployment pattern, quite a lot of software is needed to execute this. MLOps tools can help with implementing these deployment patterns or you can implement it yourself. One of the most useful frameworks for thinking about how to deploy a system is to think about deployment not as a 0, 1 is either deploy or not deploy, but instead to design a system thinking about what is the appropriate degree of automation.</p> <p> <pre><code>flowchart LR\n    %% Colors %%\n    classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff\n    classDef green fill:#16b552,stroke:#000,stroke-width:2px,color:#fff\n\n    H(Human only) ==&gt; S(Shadow mode) ==&gt; AI(AI assistance) ==&gt; PA(Partial Automation) ==&gt; F(Full automation) \n</code></pre> </p> <p>in visual inspection of smartphones, one extreme would be if there's no automation, so the human only system. </p> <p>Slightly mode automated would be if your system is running a shadow mode. So your learning algorithms are putting predictions, but it's not actually used in the factory. So that would be shadow mode. </p> <p>A slightly greater degree of automation would be AI assistance in which given a picture like this of a smartphone, you may have a human inspector make the decision. But maybe an AI system can affect the user interface to highlight the regions where there's a scratch to help draw the person's attention to where it may be most useful for them to look. The user interface or UI design is critical for human assistance. But this could be a way to get a slightly greater degree of automation while still keeping the human in the loop.</p> <p>And even greater degree of automation maybe partial automation, where given a smartphone, if the learning algorithm is sure it's fine, then that's its decision. It is sure it's defective, then we just go to algorithm's decision. But if the learning algorithm is not sure, in other words, if the learning algorithm prediction is not too confident, 0 or 1, maybe only then do we send this to a human. So this would be partial automation. Where if the learning algorithm is confident of its prediction, we go the learning algorithm. But for the hopefully small subset of images where the algorithm is not sure we send that to a human to get their judgment. And the human judgment can also be very valuable data to feedback to further train and improve the algorithm. You may find that this partial automation is sometimes a very good design point for applications where the learning algorithms performance isn't good enough for full automation. </p> <p>And then of course beyond partial automation, there is full automation where we might have the learning algorithm make every single decision. </p> <p>So there is a spectrum of using only human decisions on the left, all the way to using only the AI system's decisions on the right. And many deployment applications will start from the left and gradually move to the right. And you do not have to get all the way to full automation. You could choose to stop using AI assistance or partial automation or you could choose to go to full automation depending on the performance of your system and the needs of the application.</p>"},{"location":"MLOps/deployment/key-challenges/","title":"Key Challenges in MLOps","text":"<p>One of the most exciting thing of a Machine Learning based projects is the deployment of the models. Mainly two types of challenges make this process hard. - Concept Drift and Data Drift (a.k.a Model Drift) - Software Engineering Issues</p>"},{"location":"MLOps/deployment/key-challenges/#model-drift","title":"Model Drift","text":"<p>We have already discussed this Model Drift concepts already. You can read that from here. In this article, we will discuss the second point of the key challenges.</p>"},{"location":"MLOps/deployment/key-challenges/#software-engineering-issues","title":"Software Engineering Issues","text":"<p>When you go to the deployment of your model, you want to build a <code>prediction service</code> which takes \\(x\\) as arguments, and as prediction \\(y\\) is the outcome of the system. That can rises different points. These are</p>"},{"location":"MLOps/deployment/key-challenges/#realtime-or-batch","title":"Realtime or Batch","text":"<p>While deploying, do you need real time predictions or batch predictions ok? For example, if you are building a speech recognition system, where the user speaks and you need to get a response back, in half a second, then clearly you need real time predictions. In contrast, I have also built systems, for hospitals that take patient records. Take electronic health records and run an overnight batch process to see if there's something associated with the patients, that we can spot. So in that type of system, it was fine if we just ran it, in a batch of patient records once per night. Whether you need to write real time software, they can respond within hundreds of milliseconds or whether you can write software that just does a lot of computation overnight, that will affect how you implement your software.</p>"},{"location":"MLOps/deployment/key-challenges/#cloud-vs-edgebrowser","title":"Cloud vs Edge/Browser","text":"<p>Does your prediction service run into clouds or does it run at the edge or maybe even in a Web browser? Today there are many speech recognition systems that run in the cloud, because having the compute resources of the cloud, allows for more accurate speech recognition. There are also some speech systems, for example, a lot of speech systems within cars, actually run at the edge. There are also some mobile speech recognition systems that work, even if your Wi-Fi is turned off. Those would be examples of speech systems that run at the edge. If you use cloud for your system and if somehow the internet goes down, then this can effect to your software. In edge devices, there will be no such issues. But with the cloud, you can get more resources to help to more accurate predictions. </p>"},{"location":"MLOps/deployment/key-challenges/#computer-resources-cpugpumemory","title":"Computer Resources (CPU/GPU/Memory)","text":"<p>When building a  prediction service, it's also useful to take into account, how much computer resources you have. There have been quite a few times where you will realize to train a neural network on a very powerful GPU, only to realize that you couldn't afford an equally powerful set of GPUs for deployments, and wound up having to do something else to compress or reduce the model complexity. So if you know how much CPU or GPU resources and maybe also how much memory resources you have for your prediction service, then that could help you choose the right software architecture. Depending on your application especially if it's real-time application, latency and throughputs such as measured in terms of <code>QPS, queries per second</code>, will be other software engineering metrics you may need to hit. In speech recognition is not uncommon to want to get an answer back to the user, within half a second or 500 milliseconds. Of this 500 millisecond budget you may be able to allocate only say, 300 milliseconds to your speech recognition. So that gives a latency requirement for your system. Throughput refers to how many queries per second do you need to handle given your compute resources, maybe given a certain number of Cloud Service. For example, if you're building a system that needs to handle 1000 queries per second, it would be useful to make sure to check out your system so that you have enough computer resources, to hit the QPS requirement.</p>"},{"location":"MLOps/deployment/key-challenges/#logging","title":"Logging","text":"<p>When building your system it may be useful to log as much of the data as possible for analysis and review as well as to provide more data for retraining your learning algorithm in the future.</p>"},{"location":"MLOps/deployment/key-challenges/#security-privacy","title":"Security &amp; Privacy","text":"<p>For different applications the required levels of security and privacy can be very different. For example, if you work on electronic health records, patient records, clearly the requirements for security and privacy were very high because patient records are very highly sensitive information. Depending on your application you might want to design in the appropriate level of security and privacy, based on how sensitive that data is and also sometimes based on regulatory requirements.</p>"},{"location":"MLOps/deployment/model-drift/","title":"Model Drift","text":""},{"location":"MLOps/deployment/model-drift/#what-is-the-model-drift","title":"What is the model drift?","text":"<p>Machine learning models are subject to entropy. Model drift is the decay in a model\u2019s predictive power due to alterations in the environment. The world is dynamic, and data is constantly changing. The performance of an ML model is as good as the data it is trained on, but these models become obsolete when the world data they are modelled upon changes or the machine loses its predictive power. Further, a change in the environment changes the relationship between the model\u2019s variables. Thus, there is a need to regularly monitor models in real-time.</p> <p>Two major aspects of machine learning are the training data and the desired outcome. Hence, we have two types of model drift: Data drift and concept drift.</p>"},{"location":"MLOps/deployment/model-drift/#data-drift","title":"Data Drift","text":"<p>Simply put, data drift occurs when the data a model is trained on changes. The change in input data or independent variable leads to poor performance of the model. Microsoft has stated data drift to be one of the top reasons model accuracy degrades over time.</p> <p>Data drift is generally a consequence of seasonal changes or changes in consumer preferences over time. For instance, educational data collected before Covid shows a lesser preference for online learning than post-covid. Similarly, the demand for lipsticks has reduced considerably after Covid while face masks became a norm. As a result, Models trained on previous data will be useless. Since the input data has changed, the distribution of the variables becomes different and confuses the model.\u00a0</p> <p>Mathematically, data drift can be defined as</p> \\[Pt1(X) \\neq Pt2(X)\\] <p>Data drift can also occur at the expense of being trained on specific data but being exposed to a wider scope of data upon production. Spam detection is a good case in point. If the training data included fewer examples of spam emails, the machine is more likely to misidentify spam emails as primary once deployed.</p> <p>Sequential analysis methods, model-based methods, and time distribution-based methods are key to identifying and overcoming data drift. Drift detection method and early DDM in sequential analysis identify the model\u2019s error rate. The model-based method uses a custom model to identify the drift. The time distribution-based methods leverage statistical distance to calculate the drift between probability distributions.\u00a0</p>"},{"location":"MLOps/deployment/model-drift/#concept-drift","title":"Concept Drift","text":"<p>Contrary to data drift, where the data changes, concept drift occurs when the model\u2019s predicted target or its statistical properties change over time. During training, the model learns a function that maps the target variable, but over time, it unlearns them or is unable to use the patterns in a new environment. For instance, since the definition of spam has evolved, models have to make adjustments. Concept drift also occurs seasonally, suddenly or gradually. For example, consumer behaviour after the Covid pandemic is a sudden drift while changes in fashion trends are gradual.</p> <p>Mathematically, concept drift can be defined as</p> \\[ Pt1(Y|X) \\neq Pt2(Y|X) \\] <p>Concept drift can be measured by continuously monitoring training data and identifying changes within the dataset relationships. Popular concept drift detection algorithms include ADWIN (ADaptive WINdowing) for streaming data and the Kolmogorov\u2013Smirnov test, the chi-squared test or adversarial validation for batched data. These are applied to the model labels, predictions and data features to identify a drift.</p>"},{"location":"MLOps/deployment/model-drift/#how-to-overcome-this-model-drift","title":"How to overcome this Model Drift?","text":"<p>Concept and data drift are a response to statistical changes in the data. Hence, approaches monitoring the model\u2019s statistical properties, predictions, and their correlation with other factors help identify the drift. But several steps need to be taken post identification to ensure the model is accurate.</p> <p>Two popular approaches are online machine learning and periodic retraininG. Online learning involves updating the model to learn in real-time. This allows the data to be sequential. This allows the models to take batches of samples simultaneously and optimise the batch of data in one go. Online machine learning allows us to update learners in real-time. In online learning the models are learned in a setting where it takes the batches of samples with the time and the learner optimises the batch of data in one go. Since these models work on the fixed parameters of a data stream, they must retain the new patterns of the data. Periodic retraining of the model is also critical. Since an ML model degrades every three months on average, retraining them on regular intervals can stop drift in its tracks.</p>"},{"location":"statistics/","title":"Index","text":"<p>Info</p> <p>Get the detailed concepts which are required to become a successful Data Scientist.</p>"},{"location":"statistics/distributions-in-statistics/","title":"Distributions in Statistics:","text":""},{"location":"statistics/distributions-in-statistics/#normal-distribution","title":"Normal Distribution","text":"\\[\\large f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}{e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2}}\\] <p>\\(f(x) = probability density function\\)</p> <p>\\(\\sigma = standard deviation\\)</p> <p>\\(\\mu = mean\\)</p> <p></p>"},{"location":"statistics/distributions-in-statistics/#standardization-z-score","title":"Standardization / Z-Score","text":"<p>To compare the data to a standard normal distribution, you subtract the mean and then divided by the standard deviation. This is called standardization or z-score.</p> \\[\\large Z \\ - Score = \\frac{x_i - \\bar{x}}{\\sigma_x}\\]"},{"location":"statistics/distributions-in-statistics/#qq-plot","title":"QQ-Plot","text":"<p>A QQ - Plot is used to visually determine how close a sample specified distribution - in this case, the normal distribution.</p> <pre><code>import scipy.stats as stats\nfig, ax = plt.subplots(figsize=(4, 4))\nnorm_sample = stats.norm.rvs(size=100)\nstats.probplot(norm_sample, plot=ax)\n</code></pre>"},{"location":"statistics/distributions-in-statistics/#long-tailed-distribution","title":"Long-Tailed Distribution","text":"<p>Sometimes, the distribution is highly skewed, such as with income data or the distribution can be discrete, as with binominal data. The long narrow portion of a frequency distribution, where relatively extreme values occur at low frequency.</p> <p></p>"},{"location":"statistics/measures-of-central-tendency/","title":"Measures of Central Tendency","text":""},{"location":"statistics/measures-of-central-tendency/#data-types-in-statistics","title":"Data Types in Statistics","text":"<p>Generally 2 types of data types we can see in Data-Science.</p> <ul> <li><code>Numeric</code>: Data that are expressed on a numeric scale.<ul> <li>Continuous: Data that can take on any value in an interval.</li> <li>Discrete: Data that can take on only integer values, such as counts</li> </ul> </li> <li><code>Categorical</code>: Data that can take on only a specofic set of values representing a set of possible categories.<ul> <li>Binary: A special case of categorical data with just two categories of values e.g., 0/1, true/false.</li> <li>Ordinal: Categorical data that has an explicit ordering.</li> </ul> </li> </ul>"},{"location":"statistics/measures-of-central-tendency/#measures-of-central-tendency_1","title":"Measures of Central Tendency","text":""},{"location":"statistics/measures-of-central-tendency/#mean","title":"Mean","text":"\\[\\large \\bar{x} = \\frac{\\sum_{i=1}^{n}{x_i}}{n}\\] <pre><code>DataFrame.mean()\n</code></pre>"},{"location":"statistics/measures-of-central-tendency/#trimmed-mean","title":"Trimmed Mean","text":"<p>A variation of the mean is a trimmed mean which you calculate by dropping a fixed number of sorted values at each end and then taking an average of the remaining values.</p> <p>The formula to compute the trimmed mean with \\(p\\) smallest and largest values omitted is: \\(\\(\\large \\bar{x} = \\frac{\\sum_{i = p+1}^{n-p}{x_i}}{n - 2p}\\)\\)</p> <pre><code>import scipy.stats as stats\n# 0.1 means drop 10% from each end.\nstats.trim_mean(column_name, 0.1)\n</code></pre>"},{"location":"statistics/measures-of-central-tendency/#weighted-mean","title":"Weighted Mean","text":"\\[\\large \\bar{x_w} = \\frac{\\sum_{i=1}^{n}{w_ix_i}}{\\sum_{i=1}^{n}{w_i}}\\] <pre><code>import numpy as np\nnp.average(DataFrame.column, weights=DataFrame.another_column)\n</code></pre>"},{"location":"statistics/measures-of-central-tendency/#median","title":"Median","text":"<p>Compared to mean, which uses all the observations, the median depends only on the values in the center of the sorted data. It is also possible to compute the weighted median.</p> <p><pre><code>DataFrame.median()\n</code></pre> For weighted median, you can use the specialized package <code>wquantiles</code>. <pre><code>wquantiles.median(DataFrame.column, weights=DataFrame.another_column)\n</code></pre></p>"},{"location":"statistics/measures-of-central-tendency/#mode","title":"Mode","text":"<p>The mode is the value - or values in case of a tie - that appears most often in the data. The mode is a simple summary statistics for categorical data, and it is generally not used for numeric data.</p>"},{"location":"statistics/measures-of-central-tendency/#expected-value","title":"Expected Value","text":"<p>l data is data in which the categories represent or can be mapped to discrete values on the same scale. A marketer for a new cloud technology, for example, offers two levels of service, one priced at $300/month and another at $50/month. The marketer offers free webinars to generate leads, and the firm figures that 5% of the attendees will sign up for the $300 service, 15% will sign up for the $50 service and rest 80% will not sign up for anything. This data can be summed up, for financial purpose, in a single \"expected value,\" which is a form of weighted mean, in which the weights are probabilities. So, expected values are:</p> \\[\\large EV = 0.05 * 300 + 0.15 * 50 + 0.80 * 0 = 22.5\\] <p>In that cloud service example, the expected value of a webinar attendee is thus $22.50 per month.</p>"},{"location":"statistics/measures-of-dispersion/","title":"Measures of Dispersion or Variability","text":""},{"location":"statistics/measures-of-dispersion/#mean-absolute-deviation-mad","title":"Mean Absolute Deviation (MAD)","text":"\\[\\large MAD = \\frac{\\sum_{i=1}^{n}|x_i - \\bar{x}|}{n}\\]"},{"location":"statistics/measures-of-dispersion/#variance-standard-deviation","title":"Variance &amp; Standard Deviation","text":"\\[\\large Variance \\ (s^2) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}\\] \\[\\large STD \\ (s) = \\sqrt{s^2} = \\sqrt{\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}}\\] <pre><code># STD calculation\nDataFrame.std()\n</code></pre>"},{"location":"statistics/measures-of-dispersion/#median-absolute-variance","title":"Median Absolute Variance","text":"\\[\\large Median \\ Absolute \\ Variance = Median(|x_1 - m|, |x_2 - m|, ..., |x_n - m|)\\] <p>where \\(m\\) is the median.</p>"},{"location":"statistics/measures-of-dispersion/#range","title":"Range","text":"<p>The difference between the largest and the smallest numbers. The minimum and maximum values themselves are useful to know and are helpful in identifying outliers. But the range is extremely sensitive to outliers and not very useful as a general measure of dispersion in the data.</p>"},{"location":"statistics/measures-of-dispersion/#percentile-iqr","title":"Percentile | IQR","text":"<p>In a dataset, the \\(P\\) th percentile is a value such that at least \\(P\\) percent of the values take on this value or less and at least \\((100-P)\\) percent of the values take on this value or more.</p> <p>The \\(median\\) is the same thing as the 50th percentile. A common measurement of variability is the difference between the 25th percentile and the 75th percentile, called Interquartile Range (IQR). For very large datasets, calculating exact percentiles can be computationally very expensive since it requires sorting all the data values.</p>"},{"location":"statistics/measures-of-dispersion/#calculate-iqr-dataframecolumn_namequantile075-dataframecolumn_namequantile025","title":"<pre><code># calculate IQR\nDataFrame[column_name].quantile(0.75) - DataFrame[column_name].quantile(0.25)\n</code></pre>","text":""},{"location":"statistics/measures-of-dispersion/#correlation","title":"Correlation","text":"<p>Variables \\(X\\) and \\(Y\\) (each with measured data) are said to be <code>positively correlated</code> if high values of \\(X\\) go with high values of $Y and low values of \\(X\\) go to with low values of $Y. If high values of $X go with low values of $Y, and vice versa, the variables are <code>negatively correlated</code>.</p> <ul> <li><code>Correlation coefficient</code>: A metric that measures the extent to which numeric variables are associated with one another (ranges from -1 to +1)</li> <li><code>Correlation matrix</code>: A table where the variables are shown on both rows and columns, and the cell values are the correlations between the variables.</li> </ul> \\[Pearson's \\ correlation \\ coefficient \\ (r) = \\frac{\\sum_{i=1 }^{n}{(x_i - \\bar{x})(y_i - \\bar{y})}}{(n-1)s_xs_y}\\] <pre><code>import seaborn as sns\nsns.heatmap(DataFrame.corr(), vmin=-1, vmax=1, cmap=sns.diverging_palette(20, 220, as_cmap=True))\n</code></pre> <p>Like the <code>mean</code> and <code>standard deviation</code>, the correlation coefficient is sensitive to outliers in the data. <code>Spearman's rho</code> and <code>Kendall's tau</code> correlation techniques are based on the rank of the data and these are robust to outliers and can handle certain types of nonlinearities.</p>"},{"location":"statistics/measures-of-dispersion/#pivot-table","title":"Pivot Table","text":"<p>Whether the correlation coefficient helps to measure between two numeric columns, pivot table helps to measure between two categorical columns.</p> <pre><code>crosstab = DataFrame.pivot_table(index=category_col_1, columns=category_col_2, aggfunc=lambda x: len(x), margins=True)\ncrosstab\n</code></pre>"},{"location":"statistics/sampling/","title":"Data Sampling","text":""},{"location":"statistics/sampling/#random-sampling","title":"Random Sampling","text":"<p>Random Sampling is the process in which each available member of the population being sampled has an equal chance of being chosen for the sample at each draw. The sample that results is called a <code>simple random sample</code>. Sampling can be down <code>with replacement</code>, in which observations are put back in the population after each draw for possible future reselection. Or it can be done <code>without replacement</code>, in which case observations once selected are unavailable for future draws.</p> <p>In <code>stratified sampling</code>, the population is divided up into <code>strata</code>, and random samples are taken from each stratum.</p>"},{"location":"statistics/sampling/#statistical-bias","title":"Statistical Bias","text":"<p>Statistical bias refers to measurement or sampling errors that are <code>systematic</code> and <code>produced by the measurement</code> or <code>sampling process</code>. </p> <p>Consider the physical process of a gun shooting at a target. It will not hit the absolute center of the target every time or even much at all. An unbiased process will produce error, but it is random and does not tend strongly in any direction. </p> <p>Bias comes in different forms, may be <code>observable</code> or <code>invisible</code>. When a result does suggest bias, it is often an indicator that a statistical or machine learning model has been misspecified or an important variable left out.</p>"},{"location":"statistics/sampling/#selection-bias","title":"Selection Bias","text":"<p>Selection bias refers to the practice of selectively choosing data - consciously or unconsciously - in a way that leads to a conclusion that is misleading or ephemeral.</p> <p><code>Data snooping</code> is extensive hunting through the data until something interesting emerges. There is a saying among statisticians: \"If you torture the data long enough, sooner or later it will confess.\"</p> <p>If you repeatedly run different models and ask different questions with a large data set, you are bound to find something interesting. This is called <code>vast search effect.</code></p>"},{"location":"statistics/sampling/#central-limit-theorem-sampling-distribution","title":"Central Limit Theorem / Sampling Distribution","text":"<p>The statistics (e.g., mean) drawn from multiple samples will resemble the familiar bell-shaped normal curve, even if the source population is not normally distributed, provided that the sample size is large enough and the departure of the data from normality is not too great.</p>"},{"location":"statistics/sampling/#standard-error","title":"Standard Error","text":"<p>The standard error is a single metric that sums up the variability in the sampling distribution for a statistics.</p> \\[Standard \\ Error \\ (SE) = \\frac{std}{\\sqrt{sample \\ size}} = \\frac{s}{\\sqrt{n}}\\] <p>Consider the following approach to measuring standard error: 1. Collect a number of brand-new samples from the population. 2. For each new sample, calculate the statistics (e.g., mean) 3. Calculate the standard deviation of the statistics computed in step 2; use this as your estimate of standard error.</p>"},{"location":"statistics/sampling/#bootstrapping","title":"Bootstrapping","text":"<p>One easy and effective way to estimate the sampling distribution of a statistics, or of model parameters is to draw additional <code>samples, with replacement</code> from the sample itself and recalculate the statistics or model for each resample. This procedure is called <code>bootstrap</code>. It does not necessarily involve any assumptions about the data or the sample statistics being normally distributed. </p> <p>The algorithm for a bootstrap resampling of the mean, for a sample of size \\(n\\), is as follows: 1. Draw a sample value, record it and then replace it. 2. Repeat \\(n\\) times. 3. Record the mean of the \\(n\\) resampled values. 4. Repeat steps 1-3 \\(R\\) times. 5. Use the \\(R\\) results to:     1. Calculate their standard deviation (this estimates sample mean standard error).     2. Procedure a histogram or boxplot.     3. Find a confidence interval.</p> <pre><code>from sklearn.utils import resample\nresults = []\nfor nrepeat in range(1000):\nsample = resample(DataFrame[column_name])\nresults.append(sample.mean())\nresults = pd.Series(results)\nprint('Bootstrap Statistics:')\nprint(f'original: {DataFrame[column_name].median()}')\nprint(f\"bias: {results.median() - DataFrame[column_name].median()}\")\nprint(f\"std. error: {results.std()}\")\n</code></pre>"},{"location":"statistics/sampling/#confidence-interval-ci","title":"Confidence Interval (CI)","text":"<p>Confidence intervals always come with a coverage level, expressed as a (high) percentage, say 90% or 95%. One way to think of a 90% confidence interval is as follows: it is the interval that encloses the central 90% of the bootstrap sampling distribution of a sample statistic. More generally, an \\(x\\%\\) confidence interval around a sample estimate should, on average, contain similar sample estimates \\(x\\%\\) of the time.</p> <p>Given a sample of size \\(n\\), and a sample statistic of interest, the algorithm for a bootstrap confidence interval is as follows: 1. Draw a random sample of size \\(n\\) with replacement from the data (a resample). 2. Record the statistics of interest for the resample. 3. Repeat steps 1-2 many (\\(R\\)) times. 4. For an \\(x\\%\\) confidence interval, trim \\([(100-x)/2]\\%\\) of the \\(R\\) resample results from either end of the distribution. 5. The trim points are the endpoints of an \\(x\\%\\) bootstrap confidence interval.</p> <p>The higher the level of confidence, the wider the interval. Also, the smaller the sample, the wider the interval (i.e., the greater the uncertainty).</p>"},{"location":"statistics/statistical-tests/","title":"Statistical Tests","text":""},{"location":"statistics/statistical-tests/#ab-testing","title":"A/b Testing","text":"<p>An A/B test is an experiment with two groups to establish which of two treatments, products, procedures or the like is superior. Often one of the two treatments is the standard existing treatment or no treatment. If a standard (or no) treatment is used, it is called the <code>control</code>. A typical hypothesis is that a new treatment is better than the control. Some examples of A/B testing include: - Testing two soil treatments to determine which produces better seed germination. - Testing two therapies to determine which suppresses cancer more effectively. - Testing two prices to determine which yields more net profit. - Testing two web headlines to determine which produces more clicks. - Testing two web ads to determine which generates more conversions.</p> <p>You also need to pay attention to the <code>test statistic</code> or <code>metric</code> you use to compare group A to the group B. Perhaps the most common metric in Data Science is a binary variable: click or no-click, buy or don't buy etc. An example can be:</p> <p> Outcome Price A Price B Conversion 200 182 No conversion 23_539 22_406 <p></p> <p>If the metric is a continuous variable (purchase amount, profit etc) or a count (e.g., days in hospital, pages visited), the result might be displayed differently. If one were interested not in conversion but in revenue per page view, the results of the price test might look like (this below in typical default software output):</p> <p>Revenue/page view with price A: mean = 3.87, SD = 51.10</p> <p>Revenue/page view with price B: mean = 4.11, SD = 62.98</p>"},{"location":"statistics/statistical-tests/#why-have-a-control-group","title":"Why have a Control Group?","text":"<p>Why not skip the control group and just run an experiment applying the treatment of interest to only one group, and compare to prior experience?</p> <p>Without a control group, there is no assurance that \"all other things are equal\" and that any difference is really due to the treatment (or to chance). When you have a control group, it is subject to the same conditions (except for the treatment of interest) as the treatment group. If you simply make a comparison to \"baseline\" or prior experience, other factors, besides the treatment, might differ.</p>"},{"location":"statistics/statistical-tests/#why-just-ab-why-not-c-d","title":"Why just A/B? Why Not C, D?","text":"<p>A/B tests are popular in the marketing and e-commerce worlds, but are far from the only type of statistical experiment. Additional treatments can be included. Subjects might have repeated measurements taken. Pharmaceutical trials where subjects are scarce, expensive and acquired over time are sometimes designed with multiple opportunities to stop the experiment and reach a conclusion.</p>"},{"location":"statistics/statistical-tests/#hypothesis-tests","title":"Hypothesis Tests","text":"<p>An A/B test is typically constructed with a hypothesis in mind. For example, the hypothesis might be that price B produces higher profit. Why do we need a hypothesis? Why not just look at the outcome of the experiment and go with whichever treatment does better? The answer lies in the tendency of the human mind to underestimate the scope of natural random behavior. One manifestation of this is the failure to anticipate extreme events, or so-called \"<code>black swans</code>\". Another manifestation is the tendency to misinterpret random events as having patterns of some significance. Statistical hypothesis testing was invented as a way to protect researchers from being fooled by random chances.</p> <p>In a properly designed A/B test, you collect data on treatments A and B in such a way that any observed difference between A and B must be due to either: - Random chance in assignment of subjects - A true difference between A and B</p> <p>A statistical hypothesis test is further analysis of an A/B test, or any randomized experiment, to assess whether random chance is a reasonable explanation for the observed difference between groups A and B.</p>"},{"location":"statistics/statistical-tests/#null-alternative-hypothesis","title":"Null &amp; Alternative Hypothesis","text":"<p>Hypothesis tests use the following logic: \"Given the human tendency to react to unusual but random behavior and interpret it as something meaningful and real, in our experiments we will require proof that the difference between groups is more extreme than what chance reasonably produce.\" This involves a baseline assumption that the treatments are equivalent and any difference between the groups is due to chance. This baseline assumption is termed the <code>null hypothesis</code> \\((H_0)\\). Our hope, then, is that we can in fact prove the null hypothesis <code>wrong</code> and show that the out-comes for groups A and B are more different than what chance might produce.</p> <p>Hypothesis test by their nature involve just a null hypothesis but also an offsetting <code>alternative hypothesis</code> \\((H_A)\\). Here are some examples:</p> <ul> <li>Null = \"no difference between the means of group A and group B\"; alternative = \"A is different from B\" (could be bigger or smaller)</li> <li>Null = \"A &lt;= B\"; alternative = \"A &gt; B\"</li> <li>Null =\"B is not x% greater than A\"; alternative = \"B is X% greater than A\"</li> </ul> <p>Taken together, the null and alternative hypothesis must account for all possibilities. The nature of the null hypothesis determines the structure of the hypothesis test.</p>"},{"location":"statistics/statistical-tests/#one-way-vs-two-way-hypothesis-tests","title":"One-Way vs Two-Way Hypothesis Tests","text":"<p>Often in an A/B test, you are testing a new option (say, B) against an established default option (A), and the presumption is that you will stick with the default option unless the new option proves itself definitively better. In such a case, you want a hypothesis test to protect you from being fooled by chance in the direction favoring B. You don't care about being fooled by chance in the other direction, because you would be sticking with A unless B proves definitively better. So you want a directional alternative hypothesis (B is better than A). In such a case, you use a <code>one-way or one-tail hypothesis test</code>. This means that extreme chance results in only one direction count toward the <code>p-value</code>.</p> <p>If you want a hypothesis test to protect you from being fooled by chance in either direction, the alternative hypothesis is bidirectional (A is different from B; could be bigger or smaller). In such a case, you use a <code>two-way or two-tail hypothesis test</code>. This means that extreme chance results in either direction count toward the <code>p-value</code>.</p>"},{"location":"statistics/statistical-tests/#permutation-test","title":"Permutation Test","text":"<p>In a Permutation procedure, two or more samples are involved, typically the groups in an A/B or other hypothesis test. Permute means to change the order of a set of values. The permutation procedure is as follows:</p> <ol> <li>Combine the results (of A/B test) from the different groups A and B (and if used, C, D, ..) in a single data set.</li> <li>Shuffle the combined data and then randomly draw (without replacement) a resample of the same size as group A (clearly it will contain some data from the other groups).</li> <li>From the remaining data, randomly draw (without replacement) a resample of the same size as group B.</li> <li>Do the same for groups C, D, and so on. You have now collected one set of resamples that mirror the sizes of the original samples.</li> <li>Whatever statistics or estimate was calculated for the original samples (e.g., difference in group proportions), calculate it now for the resamples and record; this constitutes one permutation iteration.</li> <li>Repeat the previous steps \\(R\\) times to yield a permutation distribution of the test statistics.</li> </ol> <p>Now go back to the observation difference between groups and compare it to the set of permuted differences. If the observed difference lies well within the set of permuted differences, then we have not proven anything - the observed difference is within the range of what chance might produce. However, if the observed difference lies outside most of the permutation distribution, then we conclude that chance is not responsible. In technical term, the difference is <code>statistical significant</code>.</p> <p>There are two variants of the Permutation Test:</p> <ol> <li>An Exhaustive Permutation Test</li> <li>A Bootstrap Permutation Test</li> </ol> <p>We will look these topic later.</p>"},{"location":"statistics/visualization/","title":"Visualization","text":""},{"location":"statistics/visualization/#boxplot","title":"Boxplot","text":"<pre><code>DataFrame[column_name].plot.box()\n</code></pre> <p>Boxplots are a simple way to visually compare the distributions of a numeric variable grouped accordingly to a categorical variable. The pandas boxplot method takes the <code>by</code> argument that splits the data set into groups and creates the individual boxplots:</p> <pre><code>ax = DataFrame.boxplot(by=category_col_1, column=numeric_col_1)\nax.set_xlabel(category_col_1)\nax.set_ylabel(numeric_col_1)\n</code></pre> <p></p>"},{"location":"statistics/visualization/#histogram","title":"Histogram","text":"<p><pre><code>ax = DataFrame[column_name].plot.hist(figsize=(4,4))\nax.set_xlabel(\"Some label based on X-Axis)\n</code></pre> </p>"},{"location":"statistics/visualization/#densitykde-plot","title":"Density/KDE Plot","text":"<pre><code>ax = DataFrame[column_name].plot.hist(density=True)\nDataFrame[column_name].plot.density(ax=ax)\nax.set_xlabel(\"Set some label of X-axis)\n</code></pre>"},{"location":"statistics/visualization/#bar-chart","title":"Bar chart","text":"<p>Note that a bar chart resembles a histogram; in a bar chart the x-axis represents different categories of a factor variable, while in a histogram the x-axis represents values of a single variable on a numeric scale. In a histogram, the bars are typically shown touching each other, with gaps indicating values that did not occur in the data. In a bar chart, the bars are shown separate from one another.</p> <p><pre><code>ax = DataFrame[column_name].plot.bar(figsize = (4, 4), legend=False)\nax.set_xlable(\"Set some x-axis label\")\nax.set_ylabel(\"Count\")\n</code></pre> </p>"},{"location":"statistics/visualization/#scatter-plots","title":"Scatter plots","text":"<p>The standard way to visualize the relationship between two measured data variables is with a scatter plot. The x-axis represents one variables and the y-axis represents another, each point on the graph is a record.</p> <p>Scatter plots are fine when there is a relatively small number of data values. For data sets with hundred of thousands or millions of records, a scatter plot will be too dense to identify the details of the data. </p> <pre><code>ax = DataFrame.plot.scatter(x=column_name_1, y=column_name_2, figsize=(4, 4), marker='$\\u25EF$')\nax.set_xlabel(column_name_1)\nax.set_ylabel(column_name_2)\nax.axhline(0, color=\"grey\", lw=1)\nax.axvline(0, color=\"grey\", lw=1)\n</code></pre> <p></p>"},{"location":"statistics/visualization/#hexagonal-binning-plot","title":"Hexagonal Binning Plot","text":"<p>This plot is the solution of the disadvantages of scatter plot. Rather than plotting points, which would appear as a monolithic dark cloud, we can group the records into hexagonal bins and then can plot that hexagons with a color indicating the number of records in that bin.</p> <pre><code>ax = DataFrame.plot.hexbin(x=column_name_1, y=column_name_2, gridsize=30, sharex=False, figsize=(5, 4))\nax.set_xlabel(column_name_1)\nax.set_ylabel(column_name_2)\n</code></pre> <p></p>"},{"location":"statistics/visualization/#contour-plot","title":"Contour Plot","text":"<p>Another solution of the disadvantages of scatter plot. The contour plot overlaid onto a scatter plot to visualize the relationship between two numeric variables. The contours are essentially a topographical map to two variables; each contour band represents a specific density of points, increasing as one nears a \"peak\".</p> <pre><code>import seaborn as sns\nax = sns.kdeplot(DataFrame[column_name_1], DataFrame[column_name_2], ax=ax)\nax.set_xlabel(column_name_1)\nax.set_ylabel(column_name_2)\n</code></pre> <p></p>"},{"location":"statistics/visualization/#heat-maps","title":"Heat Maps","text":"<p>Another solution of the disadvantages of the scatter plot.</p> <p></p>"},{"location":"statistics/visualization/#violin-plot","title":"Violin Plot","text":"<p>A violin plot is an enhancement to the boxplot and plots the density estimate with the density on the y-axis. The density is mirrored and flipped over, and the resulting shape is filled in, creating an image resembling a violin. The advantage of a violin plot is that it can show nuances in the distribution that aren't perceptible in a boxplot. On the other hand, the boxplot more clearly shows the outliers in the data.</p> <pre><code>import seaborn as sns\nax = sns.violineplot(DataFrame[categorical_col_1], DataFrame[numerical_col_1], inner=\"quartile\", color=\"white\")\nax.set_xlabel(categorical_col_1)\nax.set_ylabel(numerical_col_1)\n</code></pre> <p></p>"}]}