{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"About"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"Details/","text":"EDA Approach: Index: Data Types in Statistics Measures of Central Tendency Mean Trimmed Mean Weighted Mean Median Mode Expected Value Measures of Dispersion Mean Absolute Deviation (MAD) Variance & Standard Deviation Mean Absolute Variance Range Percentile & IQR Visualization Boxplot Histogram Density/KDE Plot Bar chart Scatter plot Hexagonal Binning Plot Contour Plot Heat Maps Violin Plot Correlation Pivot Table Some more Statistical Concepts Random Sampling Statistical Bias Selection Bias Central Limit Theorem / Sampling Distribution Standard Error Bootstrapping Confidence Interval (CI) Data Types in Statistics: Go to top Generally 2 types of data types we can see in Data-Science. - Numeric : Data that are expressed on a numeric scale. - Continuous : Data that can take on any value in an interval. - Discrete: Data that can take on only integer values, such as counts - Categorical : Data that can take on only a specofic set of values representing a set of possible categories. - Binary : A special case of categorical data with just two categories of values e.g., 0/1, true/false. - Ordinal : Categorical data that has an explicit ordering. Measures of Central Tendency/Estimates of Location: Go to top Mean: Go to top $$\\large \\bar{x} = \\frac{\\sum_{i=1}^{n}{x_i}}{n}$$ DataFrame.mean() Trimmed Mean: Go to top A variation of the mean is a trimmed mean which you calculate by dropping a fixed number of sorted values at each end and then taking an average of the remaining values. The formula to compute the trimmed mean with $p$ smallest and largest values omitted is: $$\\large \\bar{x} = \\frac{\\sum_{i = p+1}^{n-p}{x_i}}{n - 2p}$$ import scipy.stats as stats # 0.1 means drop 10% from each end. stats.trim_mean(column_name, 0.1) Weighted Mean : Go to top $$\\large \\bar{x_w} = \\frac{\\sum_{i=1}^{n}{w_ix_i}}{\\sum_{i=1}^{n}{w_i}}$$ import numpy as np np.average(DataFrame.column, weights=DataFrame.another_column) Median: Go to top Compared to mean, which uses all the observations, the median depends only on the values in the center of the sorted data. It is also possible to compute the weighted median . DataFrame.median() For weighted median, you can use the specialized package wquantiles . wquantiles.median(DataFrame.column, weights=DataFrame.another_column) Mode: Go to top The mode is the value - or values in case of a tie - that appears most often in the data. The mode is a simple summary statistics for categorical data, and it is generally not used for numeric data. Expected Value: Go to top l data is data in which the categories represent or can be mapped to discrete values on the same scale. A marketer for a new cloud technology, for example, offers two levels of service, one priced at $300/month and another at $50/month. The marketer offers free webinars to generate leads, and the firm figures that 5% of the attendees will sign up for the $300 service, 15% will sign up for the $50 service and rest 80% will not sign up for anything. This data can be summed up, for financial purpose, in a single \" expected value ,\" which is a form of weighted mean, in which the weights are probabilities. So, expected values are: $\\large EV = 0.05 * 300 + 0.15 * 50 + 0.80 * 0 = 22.5$ In that cloud service example, the expected value of a webinar attendee is thus $22.50 per month. Measures of Dispersion/Variability: Go to top Mean Absolute Deviation (MAD): Go to top $$\\large MAD = \\frac{\\sum_{i=1}^{n}|x_i - \\bar{x}|}{n}$$ Variance & Standard Deviation: Go to top $$\\large Variance \\ (s^2) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}$$ $$\\large STD \\ (s) = \\sqrt{s^2} = \\sqrt{\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}}$$ # STD calculation DataFrame.std() Median Absolute Variance: Go to top $$\\large Median \\ Absolute \\ Variance = Median(|x_1 - m|, |x_2 - m|, ..., |x_n - m|)$$ where $m$ is the median. Range: Go to top The difference between the largest and the smallest numbers. The minimum and maximum values themselves are useful to know and are helpful in identifying outliers. But the range is extremely sensitive to outliers and not very useful as a general measure of dispersion in the data. Percentile | IQR: Go to top In a dataset, the $P$ th percentile is a value such that at least $P$ percent of the values take on this value or less and at least $(100-P)$ percent of the values take on this value or more. The $median$ is the same thing as the 50th percentile. A common measurement of variability is the difference between the 25th percentile and the 75th percentile, called Interquartile Range (IQR) . For very large datasets, calculating exact percentiles can be computationally very expensive since it requires sorting all the data values. # calculate IQR DataFrame[column_name].quantile(0.75) - DataFrame[column_name].quantile(0.25) Visualization: Go to top Boxplot: Go to top DataFrame[column_name].plot.box() Boxplots are a simple way to visually compare the distributions of a numeric variable grouped accordingly to a categorical variable. The pandas boxplot method takes the by argument that splits the data set into groups and creates the individual boxplots: ax = DataFrame.boxplot(by=category_col_1, column=numeric_col_1) ax.set_xlabel(category_col_1) ax.set_ylabel(numeric_col_1) Histogram: Go to top ax = DataFrame[column_name].plot.hist(figsize=(4,4)) ax.set_xlabel(\"Some label based on X-Axis) Density/KDE Plot: Go to top ax = DataFrame[column_name].plot.hist(density=True) DataFrame[column_name].plot.density(ax=ax) ax.set_xlabel(\"Set some label of X-axis) Bar chart: Go to top Note that a bar chart resembles a histogram; in a bar chart the x-axis represents different categories of a factor variable, while in a histogram the x-axis represents values of a single variable on a numeric scale. In a histogram, the bars are typically shown touching each other, with gaps indicating values that did not occur in the data. In a bar chart, the bars are shown separate from one another. ax = DataFrame[column_name].plot.bar(figsize = (4, 4), legend=False) ax.set_xlable(\"Set some x-axis label\") ax.set_ylabel(\"Count\") Scatter plots: Go to top The standard way to visualize the relationship between two measured data variables is with a scatter plot. The x-axis represents one variables and the y-axis represents another, each point on the graph is a record. Scatter plots are fine when there is a relatively small number of data values. For data sets with hundred of thousands or millions of records, a scatter plot will be too dense to identify the details of the data. ax = DataFrame.plot.scatter(x=column_name_1, y=column_name_2, figsize=(4, 4), marker='$\\u25EF$') ax.set_xlabel(column_name_1) ax.set_ylabel(column_name_2) ax.axhline(0, color=\"grey\", lw=1) ax.axvline(0, color=\"grey\", lw=1) Hexagonal Binning Plot: Go to top This plot is the solution of the disadvantages of scatter plot. Rather than plotting points, which would appear as a monolithic dark cloud, we can group the records into hexagonal bins and then can plot that hexagons with a color indicating the number of records in that bin. ax = DataFrame.plot.hexbin(x=column_name_1, y=column_name_2, gridsize=30, sharex=False, figsize=(5, 4)) ax.set_xlabel(column_name_1) ax.set_ylabel(column_name_2) Contour Plot: Go to top Another solution of the disadvantages of scatter plot. The contour plot overlaid onto a scatter plot to visualize the relationship between two numeric variables. The contours are essentially a topographical map to two variables; each contour band represents a specific density of points, increasing as one nears a \"peak\". import seaborn as sns ax = sns.kdeplot(DataFrame[column_name_1], DataFrame[column_name_2], ax=ax) ax.set_xlabel(column_name_1) ax.set_ylabel(column_name_2) Heat Maps: Go to top Another solution of the disadvantages of the scatter plot. Violin Plot: Go to top A violin plot is an enhancement to the boxplot and plots the density estimate with the density on the y-axis. The density is mirrored and flipped over, and the resulting shape is filled in, creating an image resembling a violin. The advantage of a violin plot is that it can show nuances in the distribution that aren't perceptible in a boxplot. On the other hand, the boxplot more clearly shows the outliers in the data. import seaborn as sns ax = sns.violineplot(DataFrame[categorical_col_1], DataFrame[numerical_col_1], inner=\"quartile\", color=\"white\") ax.set_xlabel(categorical_col_1) ax.set_ylabel(numerical_col_1) Correlation: Go to top Variables $X$ and $Y$ (each with measured data) are said to be positively correlated if high values of $X$ go with high values of $Y and low values of $X$ go to with low values of $Y. If high values of $X go with low values of $Y, and vice versa, the variables are negatively correlated . Correlation coefficient : A metric that measures the extent to which numeric variables are associated with one another (ranges from -1 to +1) Correlation matrix : A table where the variables are shown on both rows and columns, and the cell values are the correlations between the variables. $$Pearson's \\ correlation \\ coefficient \\ (r) = \\frac{\\sum_{i=1 }^{n}{(x_i - \\bar{x})(y_i - \\bar{y})}}{(n-1)s_xs_y}$$ import seaborn as sns sns.heatmap(DataFrame.corr(), vmin=-1, vmax=1, cmap=sns.diverging_palette(20, 220, as_cmap=True)) Like the mean and standard deviation , the correlation coefficient is sensitive to outliers in the data. Spearman's rho and Kendall's tau correlation techniques are based on the rank of the data and these are robust to outliers and can handle certain types of nonlinearities. Pivot Table: Go to top Whether the correlation coefficient helps to measure between two numeric columns, pivot table helps to measure between two categorical columns. crosstab = DataFrame.pivot_table(index=category_col_1, columns=category_col_2, aggfunc=lambda x: len(x), margins=True) crosstab Some more Statistical Concepts: Go to top Random Sampling Go to top Random Sampling is the process in which each available member of the population being sampled has an equal chance of being chosen for the sample at each draw. The sample that results is called a simple random sample . Sampling can be down with replacement , in which observations are put back in the population after each draw for possible future reselection. Or it can be done without replacement , in which case observations once selected are unavailable for future draws. In stratified sampling , the population is divided up into strata , and random samples are taken from each stratum. Statistical Bias: Go to top Statistical bias refers to measurement or sampling errors that are systematic and produced by the measurement or sampling process . Consider the physical process of a gun shooting at a target. It will not hit the absolute center of the target every time or even much at all. An unbiased process will produce error, but it is random and does not tend strongly in any direction. Bias comes in different forms, may be observable or invisible . When a result does suggest bias, it is often an indicator that a statistical or machine learning model has been misspecified or an important variable left out. Selection Bias: Go to top Selection bias refers to the practice of selectively choosing data - consciously or unconsciously - in a way that leads to a conclusion that is misleading or ephemeral. Data snooping is extensive hunting through the data until something interesting emerges. There is a saying among statisticians: \"If you torture the data long enough, sooner or later it will confess.\" If you repeatedly run different models and ask different questions with a large data set, you are bound to find something interesting. This is called vast search effect. Central Limit Theorem / Sampling Distribution: Go to top The statistics (e.g., mean) drawn from multiple samples will resemble the familiar bell-shaped normal curve, even if the source population is not normally distributed, provided that the sample size is large enough and the departure of the data from normality is not too great. Standard Error: Go to top The standard error is a single metric that sums up the variability in the sampling distribution for a statistics. $$Standard \\ Error \\ (SE) = \\frac{std}{\\sqrt{sample \\ size}} = \\frac{s}{\\sqrt{n}}$$ Consider the following approach to measuring standard error: 1. Collect a number of brand-new samples from the population. 2. For each new sample, calculate the statistics (e.g., mean) 3. Calculate the standard deviation of the statistics computed in step 2; use this as your estimate of standard error. Bootstrapping: Go to top One easy and effective way to estimate the sampling distribution of a statistics, or of model parameters is to draw additional samples, with replacement from the sample itself and recalculate the statistics or model for each resample. This procedure is called bootstrap . It does not necessarily involve any assumptions about the data or the sample statistics being normally distributed. The algorithm for a bootstrap resampling of the mean, for a sample of size $n$, is as follows: 1. Draw a sample value, record it and then replace it. 2. Repeat $n$ times. 3. Record the mean of the $n$ resampled values. 4. Repeat steps 1-3 $R$ times. 5. Use the $R$ results to: 1. Calculate their standard deviation (this estimates sample mean standard error). 2. Procedure a histogram or boxplot. 3. Find a confidence interval. from sklearn.utils import resample results = [] for nrepeat in range(1000): sample = resample(DataFrame[column_name]) results.append(sample.mean()) results = pd.Series(results) print('Bootstrap Statistics:') print(f'original: {DataFrame[column_name].median()}') print(f\"bias: {results.median() - DataFrame[column_name].median()}\") print(f\"std. error: {results.std()}\") Confidence Interval (CI): Go to top Confidence intervals always come with a coverage level, expressed as a (high) percentage, say 90% or 95%. One way to think of a 90% confidence interval is as follows: it is the interval that encloses the central 90% of the bootstrap sampling distribution of a sample statistic. More generally, an $x\\%$ confidence interval around a sample estimate should, on average, contain similar sample estimates $x\\%$ of the time. Given a sample of size $n$, and a sample statistic of interest, the algorithm for a bootstrap confidence interval is as follows: 1. Draw a random sample of size $n$ with replacement from the data (a resample). 2. Record the statistics of interest for the resample. 3. Repeat steps 1-2 many ($R$) times. 4. For an $x\\%$ confidence interval, trim $[(100-x)/2]\\%$ of the $R$ resample results from either end of the distribution. 5. The trim points are the endpoints of an $x\\%$ bootstrap confidence interval. The higher the level of confidence, the wider the interval. Also, the smaller the sample, the wider the interval (i.e., the greater the uncertainty). Normal Distribution: Go to top $$\\large f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}{e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2}}$$ $f(x) = probability density function$ $\\sigma = standard deviation$ $\\mu = mean$ Standardization / Z-Score Go to top To compare the data to a standard normal distribution, you subtract the mean and then divided by the standard deviation. This is called standardization or z-score . $$\\large Z \\ - Score = \\frac{x_i - \\bar{x}}{\\sigma_x}$$ QQ-Plot: Go to top A QQ - Plot is used to visually determine how close a sample specified distribution - in this case, the normal distribution. import scipy.stats as stats fig, ax = plt.subplots(figsize=(4, 4)) norm_sample = stats.norm.rvs(size=100) stats.probplot(norm_sample, plot=ax) Long-Tailed Distribution: Sometimes, the distribution is highly skewed, such as with income data or the distribution can be discrete, as with binominal data. The long narrow portion of a frequency distribution, where relatively extreme values occur at low frequency.","title":"Details"},{"location":"Details/#eda-approach","text":"","title":"EDA Approach:"},{"location":"Details/#index","text":"Data Types in Statistics Measures of Central Tendency Mean Trimmed Mean Weighted Mean Median Mode Expected Value Measures of Dispersion Mean Absolute Deviation (MAD) Variance & Standard Deviation Mean Absolute Variance Range Percentile & IQR Visualization Boxplot Histogram Density/KDE Plot Bar chart Scatter plot Hexagonal Binning Plot Contour Plot Heat Maps Violin Plot Correlation Pivot Table Some more Statistical Concepts Random Sampling Statistical Bias Selection Bias Central Limit Theorem / Sampling Distribution Standard Error Bootstrapping Confidence Interval (CI)","title":"Index:"},{"location":"Details/#data-types-in-statistics-go-to-top","text":"Generally 2 types of data types we can see in Data-Science. - Numeric : Data that are expressed on a numeric scale. - Continuous : Data that can take on any value in an interval. - Discrete: Data that can take on only integer values, such as counts - Categorical : Data that can take on only a specofic set of values representing a set of possible categories. - Binary : A special case of categorical data with just two categories of values e.g., 0/1, true/false. - Ordinal : Categorical data that has an explicit ordering.","title":"Data Types in Statistics: Go to top"},{"location":"Details/#measures-of-central-tendencyestimates-of-location-go-to-top","text":"","title":"Measures of Central Tendency/Estimates of Location: Go to top"},{"location":"Details/#mean-go-to-top","text":"$$\\large \\bar{x} = \\frac{\\sum_{i=1}^{n}{x_i}}{n}$$ DataFrame.mean()","title":"Mean: Go to top"},{"location":"Details/#trimmed-mean-go-to-top","text":"A variation of the mean is a trimmed mean which you calculate by dropping a fixed number of sorted values at each end and then taking an average of the remaining values. The formula to compute the trimmed mean with $p$ smallest and largest values omitted is: $$\\large \\bar{x} = \\frac{\\sum_{i = p+1}^{n-p}{x_i}}{n - 2p}$$ import scipy.stats as stats # 0.1 means drop 10% from each end. stats.trim_mean(column_name, 0.1)","title":"Trimmed Mean: Go to top"},{"location":"Details/#weighted-mean-go-to-top","text":"$$\\large \\bar{x_w} = \\frac{\\sum_{i=1}^{n}{w_ix_i}}{\\sum_{i=1}^{n}{w_i}}$$ import numpy as np np.average(DataFrame.column, weights=DataFrame.another_column)","title":"Weighted Mean: Go to top"},{"location":"Details/#median-go-to-top","text":"Compared to mean, which uses all the observations, the median depends only on the values in the center of the sorted data. It is also possible to compute the weighted median . DataFrame.median() For weighted median, you can use the specialized package wquantiles . wquantiles.median(DataFrame.column, weights=DataFrame.another_column)","title":"Median: Go to top"},{"location":"Details/#mode-go-to-top","text":"The mode is the value - or values in case of a tie - that appears most often in the data. The mode is a simple summary statistics for categorical data, and it is generally not used for numeric data.","title":"Mode: Go to top"},{"location":"Details/#expected-value-go-to-top","text":"l data is data in which the categories represent or can be mapped to discrete values on the same scale. A marketer for a new cloud technology, for example, offers two levels of service, one priced at $300/month and another at $50/month. The marketer offers free webinars to generate leads, and the firm figures that 5% of the attendees will sign up for the $300 service, 15% will sign up for the $50 service and rest 80% will not sign up for anything. This data can be summed up, for financial purpose, in a single \" expected value ,\" which is a form of weighted mean, in which the weights are probabilities. So, expected values are: $\\large EV = 0.05 * 300 + 0.15 * 50 + 0.80 * 0 = 22.5$ In that cloud service example, the expected value of a webinar attendee is thus $22.50 per month.","title":"Expected Value: Go to top"},{"location":"Details/#measures-of-dispersionvariability-go-to-top","text":"","title":"Measures of Dispersion/Variability: Go to top"},{"location":"Details/#mean-absolute-deviation-mad-go-to-top","text":"$$\\large MAD = \\frac{\\sum_{i=1}^{n}|x_i - \\bar{x}|}{n}$$","title":"Mean Absolute Deviation (MAD): Go to top"},{"location":"Details/#variance-standard-deviation-go-to-top","text":"$$\\large Variance \\ (s^2) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}$$ $$\\large STD \\ (s) = \\sqrt{s^2} = \\sqrt{\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}}$$ # STD calculation DataFrame.std()","title":"Variance &amp; Standard Deviation: Go to top"},{"location":"Details/#median-absolute-variance-go-to-top","text":"$$\\large Median \\ Absolute \\ Variance = Median(|x_1 - m|, |x_2 - m|, ..., |x_n - m|)$$ where $m$ is the median.","title":"Median Absolute Variance: Go to top"},{"location":"Details/#range-go-to-top","text":"The difference between the largest and the smallest numbers. The minimum and maximum values themselves are useful to know and are helpful in identifying outliers. But the range is extremely sensitive to outliers and not very useful as a general measure of dispersion in the data.","title":"Range: Go to top"},{"location":"Details/#percentile-iqr-go-to-top","text":"In a dataset, the $P$ th percentile is a value such that at least $P$ percent of the values take on this value or less and at least $(100-P)$ percent of the values take on this value or more. The $median$ is the same thing as the 50th percentile. A common measurement of variability is the difference between the 25th percentile and the 75th percentile, called Interquartile Range (IQR) . For very large datasets, calculating exact percentiles can be computationally very expensive since it requires sorting all the data values. # calculate IQR DataFrame[column_name].quantile(0.75) - DataFrame[column_name].quantile(0.25)","title":"Percentile | IQR: Go to top"},{"location":"Details/#visualization-go-to-top","text":"","title":"Visualization: Go to top"},{"location":"Details/#boxplot-go-to-top","text":"DataFrame[column_name].plot.box() Boxplots are a simple way to visually compare the distributions of a numeric variable grouped accordingly to a categorical variable. The pandas boxplot method takes the by argument that splits the data set into groups and creates the individual boxplots: ax = DataFrame.boxplot(by=category_col_1, column=numeric_col_1) ax.set_xlabel(category_col_1) ax.set_ylabel(numeric_col_1)","title":"Boxplot: Go to top"},{"location":"Details/#histogram-go-to-top","text":"ax = DataFrame[column_name].plot.hist(figsize=(4,4)) ax.set_xlabel(\"Some label based on X-Axis)","title":"Histogram: Go to top"},{"location":"Details/#densitykde-plot-go-to-top","text":"ax = DataFrame[column_name].plot.hist(density=True) DataFrame[column_name].plot.density(ax=ax) ax.set_xlabel(\"Set some label of X-axis)","title":"Density/KDE Plot: Go to top"},{"location":"Details/#bar-chart-go-to-top","text":"Note that a bar chart resembles a histogram; in a bar chart the x-axis represents different categories of a factor variable, while in a histogram the x-axis represents values of a single variable on a numeric scale. In a histogram, the bars are typically shown touching each other, with gaps indicating values that did not occur in the data. In a bar chart, the bars are shown separate from one another. ax = DataFrame[column_name].plot.bar(figsize = (4, 4), legend=False) ax.set_xlable(\"Set some x-axis label\") ax.set_ylabel(\"Count\")","title":"Bar chart: Go to top"},{"location":"Details/#scatter-plots-go-to-top","text":"The standard way to visualize the relationship between two measured data variables is with a scatter plot. The x-axis represents one variables and the y-axis represents another, each point on the graph is a record. Scatter plots are fine when there is a relatively small number of data values. For data sets with hundred of thousands or millions of records, a scatter plot will be too dense to identify the details of the data. ax = DataFrame.plot.scatter(x=column_name_1, y=column_name_2, figsize=(4, 4), marker='$\\u25EF$') ax.set_xlabel(column_name_1) ax.set_ylabel(column_name_2) ax.axhline(0, color=\"grey\", lw=1) ax.axvline(0, color=\"grey\", lw=1)","title":"Scatter plots: Go to top"},{"location":"Details/#hexagonal-binning-plot-go-to-top","text":"This plot is the solution of the disadvantages of scatter plot. Rather than plotting points, which would appear as a monolithic dark cloud, we can group the records into hexagonal bins and then can plot that hexagons with a color indicating the number of records in that bin. ax = DataFrame.plot.hexbin(x=column_name_1, y=column_name_2, gridsize=30, sharex=False, figsize=(5, 4)) ax.set_xlabel(column_name_1) ax.set_ylabel(column_name_2)","title":"Hexagonal Binning Plot: Go to top"},{"location":"Details/#contour-plot-go-to-top","text":"Another solution of the disadvantages of scatter plot. The contour plot overlaid onto a scatter plot to visualize the relationship between two numeric variables. The contours are essentially a topographical map to two variables; each contour band represents a specific density of points, increasing as one nears a \"peak\". import seaborn as sns ax = sns.kdeplot(DataFrame[column_name_1], DataFrame[column_name_2], ax=ax) ax.set_xlabel(column_name_1) ax.set_ylabel(column_name_2)","title":"Contour Plot: Go to top"},{"location":"Details/#heat-maps-go-to-top","text":"Another solution of the disadvantages of the scatter plot.","title":"Heat Maps: Go to top"},{"location":"Details/#violin-plot-go-to-top","text":"A violin plot is an enhancement to the boxplot and plots the density estimate with the density on the y-axis. The density is mirrored and flipped over, and the resulting shape is filled in, creating an image resembling a violin. The advantage of a violin plot is that it can show nuances in the distribution that aren't perceptible in a boxplot. On the other hand, the boxplot more clearly shows the outliers in the data. import seaborn as sns ax = sns.violineplot(DataFrame[categorical_col_1], DataFrame[numerical_col_1], inner=\"quartile\", color=\"white\") ax.set_xlabel(categorical_col_1) ax.set_ylabel(numerical_col_1)","title":"Violin Plot: Go to top"},{"location":"Details/#correlation-go-to-top","text":"Variables $X$ and $Y$ (each with measured data) are said to be positively correlated if high values of $X$ go with high values of $Y and low values of $X$ go to with low values of $Y. If high values of $X go with low values of $Y, and vice versa, the variables are negatively correlated . Correlation coefficient : A metric that measures the extent to which numeric variables are associated with one another (ranges from -1 to +1) Correlation matrix : A table where the variables are shown on both rows and columns, and the cell values are the correlations between the variables. $$Pearson's \\ correlation \\ coefficient \\ (r) = \\frac{\\sum_{i=1 }^{n}{(x_i - \\bar{x})(y_i - \\bar{y})}}{(n-1)s_xs_y}$$ import seaborn as sns sns.heatmap(DataFrame.corr(), vmin=-1, vmax=1, cmap=sns.diverging_palette(20, 220, as_cmap=True)) Like the mean and standard deviation , the correlation coefficient is sensitive to outliers in the data. Spearman's rho and Kendall's tau correlation techniques are based on the rank of the data and these are robust to outliers and can handle certain types of nonlinearities.","title":"Correlation: Go to top"},{"location":"Details/#pivot-table-go-to-top","text":"Whether the correlation coefficient helps to measure between two numeric columns, pivot table helps to measure between two categorical columns. crosstab = DataFrame.pivot_table(index=category_col_1, columns=category_col_2, aggfunc=lambda x: len(x), margins=True) crosstab","title":"Pivot Table: Go to top"},{"location":"Details/#some-more-statistical-concepts-go-to-top","text":"","title":"Some more Statistical Concepts: Go to top"},{"location":"Details/#random-sampling-go-to-top","text":"Random Sampling is the process in which each available member of the population being sampled has an equal chance of being chosen for the sample at each draw. The sample that results is called a simple random sample . Sampling can be down with replacement , in which observations are put back in the population after each draw for possible future reselection. Or it can be done without replacement , in which case observations once selected are unavailable for future draws. In stratified sampling , the population is divided up into strata , and random samples are taken from each stratum.","title":"Random Sampling Go to top"},{"location":"Details/#statistical-bias-go-to-top","text":"Statistical bias refers to measurement or sampling errors that are systematic and produced by the measurement or sampling process . Consider the physical process of a gun shooting at a target. It will not hit the absolute center of the target every time or even much at all. An unbiased process will produce error, but it is random and does not tend strongly in any direction. Bias comes in different forms, may be observable or invisible . When a result does suggest bias, it is often an indicator that a statistical or machine learning model has been misspecified or an important variable left out.","title":"Statistical Bias: Go to top"},{"location":"Details/#selection-bias-go-to-top","text":"Selection bias refers to the practice of selectively choosing data - consciously or unconsciously - in a way that leads to a conclusion that is misleading or ephemeral. Data snooping is extensive hunting through the data until something interesting emerges. There is a saying among statisticians: \"If you torture the data long enough, sooner or later it will confess.\" If you repeatedly run different models and ask different questions with a large data set, you are bound to find something interesting. This is called vast search effect.","title":"Selection Bias: Go to top"},{"location":"Details/#central-limit-theorem-sampling-distribution-go-to-top","text":"The statistics (e.g., mean) drawn from multiple samples will resemble the familiar bell-shaped normal curve, even if the source population is not normally distributed, provided that the sample size is large enough and the departure of the data from normality is not too great.","title":"Central Limit Theorem / Sampling Distribution: Go to top"},{"location":"Details/#standard-error-go-to-top","text":"The standard error is a single metric that sums up the variability in the sampling distribution for a statistics. $$Standard \\ Error \\ (SE) = \\frac{std}{\\sqrt{sample \\ size}} = \\frac{s}{\\sqrt{n}}$$ Consider the following approach to measuring standard error: 1. Collect a number of brand-new samples from the population. 2. For each new sample, calculate the statistics (e.g., mean) 3. Calculate the standard deviation of the statistics computed in step 2; use this as your estimate of standard error.","title":"Standard Error: Go to top"},{"location":"Details/#bootstrapping-go-to-top","text":"One easy and effective way to estimate the sampling distribution of a statistics, or of model parameters is to draw additional samples, with replacement from the sample itself and recalculate the statistics or model for each resample. This procedure is called bootstrap . It does not necessarily involve any assumptions about the data or the sample statistics being normally distributed. The algorithm for a bootstrap resampling of the mean, for a sample of size $n$, is as follows: 1. Draw a sample value, record it and then replace it. 2. Repeat $n$ times. 3. Record the mean of the $n$ resampled values. 4. Repeat steps 1-3 $R$ times. 5. Use the $R$ results to: 1. Calculate their standard deviation (this estimates sample mean standard error). 2. Procedure a histogram or boxplot. 3. Find a confidence interval. from sklearn.utils import resample results = [] for nrepeat in range(1000): sample = resample(DataFrame[column_name]) results.append(sample.mean()) results = pd.Series(results) print('Bootstrap Statistics:') print(f'original: {DataFrame[column_name].median()}') print(f\"bias: {results.median() - DataFrame[column_name].median()}\") print(f\"std. error: {results.std()}\")","title":"Bootstrapping: Go to top"},{"location":"Details/#confidence-interval-ci-go-to-top","text":"Confidence intervals always come with a coverage level, expressed as a (high) percentage, say 90% or 95%. One way to think of a 90% confidence interval is as follows: it is the interval that encloses the central 90% of the bootstrap sampling distribution of a sample statistic. More generally, an $x\\%$ confidence interval around a sample estimate should, on average, contain similar sample estimates $x\\%$ of the time. Given a sample of size $n$, and a sample statistic of interest, the algorithm for a bootstrap confidence interval is as follows: 1. Draw a random sample of size $n$ with replacement from the data (a resample). 2. Record the statistics of interest for the resample. 3. Repeat steps 1-2 many ($R$) times. 4. For an $x\\%$ confidence interval, trim $[(100-x)/2]\\%$ of the $R$ resample results from either end of the distribution. 5. The trim points are the endpoints of an $x\\%$ bootstrap confidence interval. The higher the level of confidence, the wider the interval. Also, the smaller the sample, the wider the interval (i.e., the greater the uncertainty).","title":"Confidence Interval (CI): Go to top"},{"location":"Details/#normal-distribution-go-to-top","text":"$$\\large f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}{e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2}}$$ $f(x) = probability density function$ $\\sigma = standard deviation$ $\\mu = mean$","title":"Normal Distribution: Go to top"},{"location":"Details/#standardization-z-score-go-to-top","text":"To compare the data to a standard normal distribution, you subtract the mean and then divided by the standard deviation. This is called standardization or z-score . $$\\large Z \\ - Score = \\frac{x_i - \\bar{x}}{\\sigma_x}$$","title":"Standardization / Z-Score Go to top"},{"location":"Details/#qq-plot-go-to-top","text":"A QQ - Plot is used to visually determine how close a sample specified distribution - in this case, the normal distribution. import scipy.stats as stats fig, ax = plt.subplots(figsize=(4, 4)) norm_sample = stats.norm.rvs(size=100) stats.probplot(norm_sample, plot=ax)","title":"QQ-Plot: Go to top"},{"location":"Details/#long-tailed-distribution","text":"Sometimes, the distribution is highly skewed, such as with income data or the distribution can be discrete, as with binominal data. The long narrow portion of a frequency distribution, where relatively extreme values occur at low frequency.","title":"Long-Tailed Distribution:"}]}