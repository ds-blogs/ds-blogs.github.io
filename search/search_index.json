{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Data Science Hub","text":"<p>Myself Sayan Roy and I am incoming Data Scientist at Celebal Technologies which is a service based company. </p>"},{"location":"#important-pages","title":"Important Pages","text":"<ul> <li><code>Basic Statistics</code> </li> </ul>"},{"location":"distributions%20in%20statistics/","title":"Distributions in Statistics:","text":""},{"location":"distributions%20in%20statistics/#normal-distribution","title":"Normal Distribution","text":"<p>  </p> <p>$f(x) = probability density function$</p> <p>$\\sigma = standard deviation$</p> <p>$\\mu = mean$</p>"},{"location":"distributions%20in%20statistics/#standardization-z-score","title":"Standardization / Z-Score","text":"<p>To compare the data to a standard normal distribution, you subtract the mean and then divided by the standard deviation. This is called standardization or z-score.</p> <p> \\large Z \\ - Score = \\frac{x_i - \\bar{x}}{\\sigma_x} </p>"},{"location":"distributions%20in%20statistics/#qq-plot","title":"QQ-Plot","text":"<p>A QQ - Plot is used to visually determine how close a sample specified distribution - in this case, the normal distribution.</p> <pre><code>import scipy.stats as stats\n\nfig, ax = plt.subplots(figsize=(4, 4))\nnorm_sample = stats.norm.rvs(size=100)\nstats.probplot(norm_sample, plot=ax)\n</code></pre>"},{"location":"distributions%20in%20statistics/#long-tailed-distribution","title":"Long-Tailed Distribution","text":"<p>Sometimes, the distribution is highly skewed, such as with income data or the distribution can be discrete, as with binominal data. The long narrow portion of a frequency distribution, where relatively extreme values occur at low frequency.</p>"},{"location":"docker%20guide/","title":"Docker Guide","text":""},{"location":"docker%20guide/#what-is-docker-image-docker-container","title":"What is Docker Image &amp; Docker Container?","text":"<p>Docker Image is an executable package of software that includes everything needed to run an application. This image informs how a container should instantiate, determining which software components will run and how. Docker Container is a virtual environment that bundles application code with all the dependencies required to run the application. The application runs quickly and reliably from one computing environment to another. One Docker Container only can hold one Docker Image.</p> <p>You can find the images from Docker Hub.</p>"},{"location":"docker%20guide/#reference-of-docker-some-important-commands","title":"Reference of Docker | Some Important Commands","text":""},{"location":"docker%20guide/#check-the-version","title":"Check the version:","text":"<p>There are 2 commands that can help to check the docker version</p> <pre><code>docker version\n</code></pre> <p>or</p> <pre><code>docker -v\n</code></pre>"},{"location":"docker%20guide/#docker-pull","title":"<code>docker pull</code>:","text":"<p>Pull an image or a repository from a registry/docker hub. Like to use postgres, you have to pull that image first by the below command:</p> <pre><code>docker pull postgres\n</code></pre> <p>When you don't mention the tag, this command will pull the latest image. If you want to pull postgres image version 14, then the command will be</p> <pre><code>docker pull postgres:14\n</code></pre> <p>Note: <code>:14</code> is called <code>tag</code>.</p>"},{"location":"docker%20guide/#docker-image","title":"<code>docker image</code>:","text":"<ul> <li>Using this command, you can see list of images are there in your local machine. This command gives you some information about the docker images.</li> </ul> <pre><code>docker image ls\n</code></pre>"},{"location":"docker%20guide/#docker-ps","title":"<code>docker ps</code>:","text":"<p>Same as <code>docker image</code> command, this command helps to work with containers. For example, to list down all the containers of your local machine, this below command is used.</p> <pre><code>docker ps\n</code></pre>"},{"location":"docker%20guide/#docker-run","title":"<code>docker run</code>:","text":"<p>One of the most used command in docker family. It helps to run the image that you have pulled or have created to your local machine. E.g.,</p> <pre><code>docker run postgres\n</code></pre> <p>or</p> <pre><code>docker run postgres:13.8\n</code></pre> <p>The above command will run the <code>latest postgres image</code>. postgres is the image name. If you want to run a specific version of the image, you have to specify as so called <code>tag</code>.</p> <p>Important options: - <code>-it</code>: This instructs docker to allocate a pseudo.TTY connected to the container's stdin; creating an interactive bash shell in the container. - <code>-e ENVIRONMENT_VAR=VALUE</code>: Some images need environment variables which help to run the image properly. E.g., to run postgres properly, you have to set password by <code>POSTGRES_PASSWORD=mysecretpassword</code>. - <code>-d</code>: Detach mode. When you run the image, it keeps your terminal busy. You can enter or command any other command or have to keep open the terminal to run the image continuously. If you pass this option, then the terminal will be detached and you can work with it. - <code>--name some-name</code>: If you don't specify this option, docker comes up a name on its own and sometimes it is confusing. For that, you can name the container. You can't name same twice or without deleting the previous container. - <code>-p host_port:container_port</code>: Some docker image(s) you run together. But it is possible that the port is same and that can conflict to your application. For that, you can open a port (host_port) to your local machine that will communicate with the container_port. Very important option. To know which port the container is using, you can use <code>docker ps</code> command. This concept is known as <code>port mapping</code>. - - <code>--net network-name</code>: Connect the container to a network. This can help to connect other containers with this containers easily. Otherwise different networks can't connect each other.</p>"},{"location":"docker%20guide/#docker-stop-or-docker-container-stop","title":"<code>docker stop</code> or <code>docker container stop</code>:","text":"<p>Stop one or more running containers.</p> <pre><code>docker stop container_name\nor\ndocker stop container_id\nor\ndocker container stop container_name\nor\ndocker container stop container_id\n</code></pre>"},{"location":"docker%20guide/#docker-container-ls","title":"<code>docker container ls</code>:","text":"<p>An alternative of the command <code>docker ps</code> to list out all the active docker containers.</p> <pre><code>docker container ls\n</code></pre> <p>If you want all the containers whatever it is running or not, command this:</p> <pre><code>docker container ls -a\n</code></pre>"},{"location":"docker%20guide/#docker-container-prune","title":"<code>docker container prune</code>:","text":"<p>Remove all the stopped containers from your local machine. This is a very dangerous command. You have to be very careful when you are using it. Though it does remove the stopped containers but does not remove the volumes associated with the containers.</p> <pre><code>docker container prune\n</code></pre>"},{"location":"docker%20guide/#docker-logs","title":"<code>docker logs</code>:","text":"<p>You can see the logs of the containers. It helps to get inside what is executing in containers.</p> <pre><code>docker logs container_name\nor\ndocker logs container_id\n</code></pre>"},{"location":"docker%20guide/#docker-example","title":"Docker Example:","text":""},{"location":"docker%20guide/#connect-docker-mongo-express-with-mongodb-docker-compose","title":"Connect docker mongo-express with mongodb | Docker Compose:","text":"<p>MongoDB is the database and the MongoDB-Express is the Web-based MongoDB admin interface, written with Node.js and express. So, we will start the container first of the MongoDB and then also start the MongoDB-Express container by connecting with the MongoDB network.</p> <p>Create a network that both containers will connect together.</p> <pre><code>docker network create mongo-network\n</code></pre> <p>Start the MongoDB container:</p> <pre><code>docker run -p 27017:27017 -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=password --name mongodb --net mongo-network -d mongo\n</code></pre> <p>Now start the MongoDB-Express container:</p> <pre><code>docker run --network mongo-network -e ME_CONFIG_MONGODB_SERVER=mongodb -e ME_CONFIG_MONGODB_ADMINUSERNAME=admin -e ME_CONFIG_MONGODB_ADMINPASSWORD=password -p 8081:8081 -d --name mongo-express mongo-express\n</code></pre> <p>Now the web app of mongodb-express container is running and you can visit by using the browser. The url is http://localhost:8081. You can visit it. If we stop the <code>mongodb</code> container that we have created, it will also stop the <code>mongo-express</code> container because <code>mongodb</code> container is the dependency of the <code>mongo-express</code> container.</p> <p>So, we have wrote lots of long commands to do this. This can cause issue if you have done a small spelling mistake. To solve this issue, <code>docker compose</code> come out. For this you can create a yml file. The file name can be anything. For our case, we can name it as docker-compose.yml.</p> <pre><code>version: '3'\nservices:\n    mongodb:\n        image: mongo\n        ports:\n            - \"27017:27017\"\n        environment:\n            - MONGO_INITDB_ROOT_USERNAME=admin\n            - MONGO_INITDB_ROOT_PASSWORD=password\n        volumes:\n            - mymongo-data:/data/db\n    mongo-express:\n        image: mongo-express\n        restart: always\n        ports:\n            - \"8081:8081\"\n        environment:\n            - ME_CONFIG_MONGODB_SERVER=mongodb\n            - ME_CONFIG_MONGODB_ADMINUSERNAME=admin\n            - ME_CONFIG_MONGODB_ADMINPASSWORD=password\nvolumes:\n    mymongo-data:\n        driver: local\n</code></pre> <p>After creating the compose yml file, you can run the file by the following command:</p> <pre><code>docker-compose -f docker-compose.yml up\n</code></pre> <p>The best thing with docker compose is that it will create a network by-default and will put all the containers into it. We don't have to specify separately in the docker compose file. Check out the <code>mongodb</code> container. One option is <code>volumes</code>. That is different with the lowers <code>volumes</code> settings. The value of<code>volumes</code> of <code>mongodb</code> container should be default value of the mongo image. That can be different with MySQL or any other image. For that, you have to look to the documentation of the images. By using the <code>volumes</code>, you can detach and attach the volume(s) with the container according to the requirements.</p> <p>To stop this compose, you can run the command to a another terminal. This will also remove the containers from your local machine automatically. You don't have to do it manually.</p> <pre><code>docker-compose -f docker-compose.yml down\n</code></pre>"},{"location":"docker%20guide/#flask-app-create-docker-image-push-image-to-docker-hub","title":"Flask APP | Create Docker Image | Push Image to Docker Hub:","text":"<p>To create an docker image, you have to create a file named <code>Dockerfile</code> inside your root project directory. According to the demo-flask app, add the below commands to the <code>Dockerfile</code>.</p> <pre><code># define the base image\nFROM python:3-alpine3.15\n# define the working directory\nWORKDIR /app\n# copy everything and move those to the directory\nCOPY . /app\n# install the required python packages using pip\nRUN pip install -r requirements.txt\n# expose the port in which this application will run.\n# This is defined to the python flask file.\nEXPOSE 3000\n# Now start the application, the entry point of your app\nCMD python ./index.py\n</code></pre> <p>Now to build the image of your flask app, you can run the below command. <code>sayanroy7</code> is the username of mine (to get your username, you have to create an account to docker hub). <code>flask-docker-demo</code> is the image of the image that you are giving. <code>:0.0.1.RELEASE</code> is the <code>flag</code> of this image.</p> <pre><code>docker build -t sayanroy7/flask-docker-demo:0.0.1.RELEASE .\n</code></pre> <p>After building the image, you can see it to your Docker Desktop app. Before pushing to the hub, check that is it working fine or not. For that, run the simple command:</p> <pre><code>docker container run -d -p 3000:3000 --name flask-docker-demo sayanroy7/flask-docker-demo:0.0.1.RELEASE\n</code></pre> <p>If you go to the browser and hit http://localhost:3000, you should get the result as</p> <pre><code>{\"message\":\"Hey there Python\"}\n</code></pre> <p>If the image is working fine, then push the image to the Docker Hub by the command. If you already signed in to your Docker Desktop, then the image will be uploaded smoothly otherwise it will prompt for the authentications.</p> <pre><code>docker push sayanroy7/flask-docker-demo:0.0.1.RELEASE \n</code></pre> <p>And in your Docker Hub, you can see the image under the Repositories tab!! </p>"},{"location":"measures%20of%20central%20tendency/","title":"Measures of Central Tendency","text":""},{"location":"measures%20of%20central%20tendency/#data-types-in-statistics","title":"Data Types in Statistics","text":"<p>Generally 2 types of data types we can see in Data-Science.</p> <ul> <li><code>Numeric</code>: Data that are expressed on a numeric scale.<ul> <li>Continuous: Data that can take on any value in an interval.</li> <li>Discrete: Data that can take on only integer values, such as counts</li> </ul> </li> <li><code>Categorical</code>: Data that can take on only a specofic set of values representing a set of possible categories.<ul> <li>Binary: A special case of categorical data with just two categories of values e.g., 0/1, true/false.</li> <li>Ordinal: Categorical data that has an explicit ordering.</li> </ul> </li> </ul>"},{"location":"measures%20of%20central%20tendency/#measures-of-central-tendency_1","title":"Measures of Central Tendency","text":""},{"location":"measures%20of%20central%20tendency/#mean","title":"Mean","text":"<p>  </p> <pre><code>DataFrame.mean()\n</code></pre>"},{"location":"measures%20of%20central%20tendency/#trimmed-mean","title":"Trimmed Mean","text":"<p>A variation of the mean is a trimmed mean which you calculate by dropping a fixed number of sorted values at each end and then taking an average of the remaining values.</p> <p>The formula to compute the trimmed mean with $p$ smallest and largest values omitted is: \\large \\bar{x} = \\frac{\\sum_{i = p+1}^{n-p}{x_i}}{n - 2p} </p> <pre><code>import scipy.stats as stats\n\n# 0.1 means drop 10% from each end.\nstats.trim_mean(column_name, 0.1)\n</code></pre>"},{"location":"measures%20of%20central%20tendency/#weighted-mean","title":"Weighted Mean","text":"<p> \\large \\bar{x_w} = \\frac{\\sum_{i=1}^{n}{w_ix_i}}{\\sum_{i=1}^{n}{w_i}} </p> <pre><code>import numpy as np\n\nnp.average(DataFrame.column, weights=DataFrame.another_column)\n</code></pre>"},{"location":"measures%20of%20central%20tendency/#median","title":"Median","text":"<p>Compared to mean, which uses all the observations, the median depends only on the values in the center of the sorted data. It is also possible to compute the weighted median.</p> <pre><code>DataFrame.median()\n</code></pre> <p>For weighted median, you can use the specialized package <code>wquantiles</code>.</p> <pre><code>wquantiles.median(DataFrame.column, weights=DataFrame.another_column)\n</code></pre>"},{"location":"measures%20of%20central%20tendency/#mode","title":"Mode","text":"<p>The mode is the value - or values in case of a tie - that appears most often in the data. The mode is a simple summary statistics for categorical data, and it is generally not used for numeric data.</p>"},{"location":"measures%20of%20central%20tendency/#expected-value","title":"Expected Value","text":"<p>l data is data in which the categories represent or can be mapped to discrete values on the same scale. A marketer for a new cloud technology, for example, offers two levels of service, one priced at $300/month and another at $50/month. The marketer offers free webinars to generate leads, and the firm figures that 5% of the attendees will sign up for the $300 service, 15% will sign up for the $50 service and rest 80% will not sign up for anything. This data can be summed up, for financial purpose, in a single \"expected value,\" which is a form of weighted mean, in which the weights are probabilities. So, expected values are:</p> <p> \\large EV = 0.05 * 300 + 0.15 * 50 + 0.80 * 0 = 22.5 </p> <p>In that cloud service example, the expected value of a webinar attendee is thus $22.50 per month.</p>"},{"location":"measures%20of%20dispersion/","title":"Measures of Dispersion or Variability","text":""},{"location":"measures%20of%20dispersion/#mean-absolute-deviation-mad","title":"Mean Absolute Deviation (MAD)","text":"<p>  </p>"},{"location":"measures%20of%20dispersion/#variance-standard-deviation","title":"Variance &amp; Standard Deviation","text":"<p> \\large Variance \\ (s^2) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n-1} </p> <p> \\large STD \\ (s) = \\sqrt{s^2} = \\sqrt{\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}} </p> <pre><code># STD calculation\nDataFrame.std()\n</code></pre>"},{"location":"measures%20of%20dispersion/#median-absolute-variance","title":"Median Absolute Variance","text":"<p> \\large Median \\ Absolute \\ Variance = Median(|x_1 - m|, |x_2 - m|, ..., |x_n - m|) </p> <p>where $m$ is the median.</p>"},{"location":"measures%20of%20dispersion/#range","title":"Range","text":"<p>The difference between the largest and the smallest numbers. The minimum and maximum values themselves are useful to know and are helpful in identifying outliers. But the range is extremely sensitive to outliers and not very useful as a general measure of dispersion in the data.</p>"},{"location":"measures%20of%20dispersion/#percentile-iqr","title":"Percentile | IQR","text":"<p>In a dataset, the $P$ th percentile is a value such that at least $P$ percent of the values take on this value or less and at least $(100-P)$ percent of the values take on this value or more.</p> <p>The $median$ is the same thing as the 50th percentile. A common measurement of variability is the difference between the 25th percentile and the 75th percentile, called Interquartile Range (IQR). For very large datasets, calculating exact percentiles can be computationally very expensive since it requires sorting all the data values.</p> <pre><code># calculate IQR\nDataFrame[column_name].quantile(0.75) - DataFrame[column_name].quantile(0.25)\n</code></pre>"},{"location":"measures%20of%20dispersion/#correlation","title":"Correlation","text":"<p>Variables $X$ and $Y$ (each with measured data) are said to be <code>positively correlated</code> if high values of $X$ go with high values of $Y and low values of $X$ go to with low values of $Y. If high values of $X go with low values of $Y, and vice versa, the variables are <code>negatively correlated</code>.</p> <ul> <li><code>Correlation coefficient</code>: A metric that measures the extent to which numeric variables are associated with one another (ranges from -1 to +1)</li> <li><code>Correlation matrix</code>: A table where the variables are shown on both rows and columns, and the cell values are the correlations between the variables.</li> </ul> <p> Pearson's \\ correlation \\ coefficient \\ (r) = \\frac{\\sum_{i=1 }^{n}{(x_i - \\bar{x})(y_i - \\bar{y})}}{(n-1)s_xs_y} </p> <pre><code>import seaborn as sns\n\nsns.heatmap(DataFrame.corr(), vmin=-1, vmax=1, cmap=sns.diverging_palette(20, 220, as_cmap=True))\n</code></pre> <p>Like the <code>mean</code> and <code>standard deviation</code>, the correlation coefficient is sensitive to outliers in the data. <code>Spearman's rho</code> and <code>Kendall's tau</code> correlation techniques are based on the rank of the data and these are robust to outliers and can handle certain types of nonlinearities.</p>"},{"location":"measures%20of%20dispersion/#pivot-table","title":"Pivot Table","text":"<p>Whether the correlation coefficient helps to measure between two numeric columns, pivot table helps to measure between two categorical columns.</p> <pre><code>crosstab = DataFrame.pivot_table(index=category_col_1, columns=category_col_2, aggfunc=lambda x: len(x), margins=True)\ncrosstab\n</code></pre>"},{"location":"sampling/","title":"Data Sampling","text":""},{"location":"sampling/#random-sampling","title":"Random Sampling","text":"<p>Random Sampling is the process in which each available member of the population being sampled has an equal chance of being chosen for the sample at each draw. The sample that results is called a <code>simple random sample</code>. Sampling can be down <code>with replacement</code>, in which observations are put back in the population after each draw for possible future reselection. Or it can be done <code>without replacement</code>, in which case observations once selected are unavailable for future draws.</p> <p>In <code>stratified sampling</code>, the population is divided up into <code>strata</code>, and random samples are taken from each stratum.</p>"},{"location":"sampling/#statistical-bias","title":"Statistical Bias","text":"<p>Statistical bias refers to measurement or sampling errors that are <code>systematic</code> and <code>produced by the measurement</code> or <code>sampling process</code>. </p> <p>Consider the physical process of a gun shooting at a target. It will not hit the absolute center of the target every time or even much at all. An unbiased process will produce error, but it is random and does not tend strongly in any direction. </p> <p>Bias comes in different forms, may be <code>observable</code> or <code>invisible</code>. When a result does suggest bias, it is often an indicator that a statistical or machine learning model has been misspecified or an important variable left out.</p>"},{"location":"sampling/#selection-bias","title":"Selection Bias","text":"<p>Selection bias refers to the practice of selectively choosing data - consciously or unconsciously - in a way that leads to a conclusion that is misleading or ephemeral.</p> <p><code>Data snooping</code> is extensive hunting through the data until something interesting emerges. There is a saying among statisticians: \"If you torture the data long enough, sooner or later it will confess.\"</p> <p>If you repeatedly run different models and ask different questions with a large data set, you are bound to find something interesting. This is called <code>vast search effect.</code></p>"},{"location":"sampling/#central-limit-theorem-sampling-distribution","title":"Central Limit Theorem / Sampling Distribution","text":"<p>The statistics (e.g., mean) drawn from multiple samples will resemble the familiar bell-shaped normal curve, even if the source population is not normally distributed, provided that the sample size is large enough and the departure of the data from normality is not too great.</p>"},{"location":"sampling/#standard-error","title":"Standard Error","text":"<p>The standard error is a single metric that sums up the variability in the sampling distribution for a statistics.</p> <p>  </p> <p>Consider the following approach to measuring standard error: 1. Collect a number of brand-new samples from the population. 2. For each new sample, calculate the statistics (e.g., mean) 3. Calculate the standard deviation of the statistics computed in step 2; use this as your estimate of standard error.</p>"},{"location":"sampling/#bootstrapping","title":"Bootstrapping","text":"<p>One easy and effective way to estimate the sampling distribution of a statistics, or of model parameters is to draw additional <code>samples, with replacement</code> from the sample itself and recalculate the statistics or model for each resample. This procedure is called <code>bootstrap</code>. It does not necessarily involve any assumptions about the data or the sample statistics being normally distributed. </p> <p>The algorithm for a bootstrap resampling of the mean, for a sample of size $n$, is as follows: 1. Draw a sample value, record it and then replace it. 2. Repeat $n$ times. 3. Record the mean of the $n$ resampled values. 4. Repeat steps 1-3 $R$ times. 5. Use the $R$ results to:     1. Calculate their standard deviation (this estimates sample mean standard error).     2. Procedure a histogram or boxplot.     3. Find a confidence interval.</p> <pre><code>from sklearn.utils import resample\n\nresults = []\nfor nrepeat in range(1000):\n    sample = resample(DataFrame[column_name])\n    results.append(sample.mean())\n\nresults = pd.Series(results)\nprint('Bootstrap Statistics:')\nprint(f'original: {DataFrame[column_name].median()}')\nprint(f\"bias: {results.median() - DataFrame[column_name].median()}\")\nprint(f\"std. error: {results.std()}\")\n</code></pre>"},{"location":"sampling/#confidence-interval-ci","title":"Confidence Interval (CI)","text":"<p>Confidence intervals always come with a coverage level, expressed as a (high) percentage, say 90% or 95%. One way to think of a 90% confidence interval is as follows: it is the interval that encloses the central 90% of the bootstrap sampling distribution of a sample statistic. More generally, an $x\\%$ confidence interval around a sample estimate should, on average, contain similar sample estimates $x\\%$ of the time.</p> <p>Given a sample of size $n$, and a sample statistic of interest, the algorithm for a bootstrap confidence interval is as follows: 1. Draw a random sample of size $n$ with replacement from the data (a resample). 2. Record the statistics of interest for the resample. 3. Repeat steps 1-2 many ($R$) times. 4. For an $x\\%$ confidence interval, trim $[(100-x)/2]\\%$ of the $R$ resample results from either end of the distribution. 5. The trim points are the endpoints of an $x\\%$ bootstrap confidence interval.</p> <p>The higher the level of confidence, the wider the interval. Also, the smaller the sample, the wider the interval (i.e., the greater the uncertainty).</p>"},{"location":"visualization/","title":"Visualization","text":""},{"location":"visualization/#boxplot","title":"Boxplot","text":"<pre><code>DataFrame[column_name].plot.box()\n</code></pre>  <p>Boxplots are a simple way to visually compare the distributions of a numeric variable grouped accordingly to a categorical variable. The pandas boxplot method takes the <code>by</code> argument that splits the data set into groups and creates the individual boxplots:</p> <pre><code>ax = DataFrame.boxplot(by=category_col_1, column=numeric_col_1)\nax.set_xlabel(category_col_1)\nax.set_ylabel(numeric_col_1)\n</code></pre>"},{"location":"visualization/#histogram","title":"Histogram","text":"<pre><code>ax = DataFrame[column_name].plot.hist(figsize=(4,4))\nax.set_xlabel(\"Some label based on X-Axis)\n</code></pre>"},{"location":"visualization/#densitykde-plot","title":"Density/KDE Plot","text":"<pre><code>ax = DataFrame[column_name].plot.hist(density=True)\nDataFrame[column_name].plot.density(ax=ax)\nax.set_xlabel(\"Set some label of X-axis)\n</code></pre>"},{"location":"visualization/#bar-chart","title":"Bar chart","text":"<p>Note that a bar chart resembles a histogram; in a bar chart the x-axis represents different categories of a factor variable, while in a histogram the x-axis represents values of a single variable on a numeric scale. In a histogram, the bars are typically shown touching each other, with gaps indicating values that did not occur in the data. In a bar chart, the bars are shown separate from one another.</p> <pre><code>ax = DataFrame[column_name].plot.bar(figsize = (4, 4), legend=False)\nax.set_xlable(\"Set some x-axis label\")\nax.set_ylabel(\"Count\")\n</code></pre>"},{"location":"visualization/#scatter-plots","title":"Scatter plots","text":"<p>The standard way to visualize the relationship between two measured data variables is with a scatter plot. The x-axis represents one variables and the y-axis represents another, each point on the graph is a record.</p> <p>Scatter plots are fine when there is a relatively small number of data values. For data sets with hundred of thousands or millions of records, a scatter plot will be too dense to identify the details of the data. </p> <pre><code>ax = DataFrame.plot.scatter(x=column_name_1, y=column_name_2, figsize=(4, 4), marker='$\\u25EF$')\nax.set_xlabel(column_name_1)\nax.set_ylabel(column_name_2)\nax.axhline(0, color=\"grey\", lw=1)\nax.axvline(0, color=\"grey\", lw=1)\n</code></pre>"},{"location":"visualization/#hexagonal-binning-plot","title":"Hexagonal Binning Plot","text":"<p>This plot is the solution of the disadvantages of scatter plot. Rather than plotting points, which would appear as a monolithic dark cloud, we can group the records into hexagonal bins and then can plot that hexagons with a color indicating the number of records in that bin.</p> <pre><code>ax = DataFrame.plot.hexbin(x=column_name_1, y=column_name_2, gridsize=30, sharex=False, figsize=(5, 4))\nax.set_xlabel(column_name_1)\nax.set_ylabel(column_name_2)\n</code></pre>"},{"location":"visualization/#contour-plot","title":"Contour Plot","text":"<p>Another solution of the disadvantages of scatter plot. The contour plot overlaid onto a scatter plot to visualize the relationship between two numeric variables. The contours are essentially a topographical map to two variables; each contour band represents a specific density of points, increasing as one nears a \"peak\".</p> <pre><code>import seaborn as sns\n\nax = sns.kdeplot(DataFrame[column_name_1], DataFrame[column_name_2], ax=ax)\nax.set_xlabel(column_name_1)\nax.set_ylabel(column_name_2)\n</code></pre>"},{"location":"visualization/#heat-maps","title":"Heat Maps","text":"<p>Another solution of the disadvantages of the scatter plot.</p>"},{"location":"visualization/#violin-plot","title":"Violin Plot","text":"<p>A violin plot is an enhancement to the boxplot and plots the density estimate with the density on the y-axis. The density is mirrored and flipped over, and the resulting shape is filled in, creating an image resembling a violin. The advantage of a violin plot is that it can show nuances in the distribution that aren't perceptible in a boxplot. On the other hand, the boxplot more clearly shows the outliers in the data.</p> <pre><code>import seaborn as sns\n\nax = sns.violineplot(DataFrame[categorical_col_1], DataFrame[numerical_col_1], inner=\"quartile\", color=\"white\")\nax.set_xlabel(categorical_col_1)\nax.set_ylabel(numerical_col_1)\n</code></pre>"}]}