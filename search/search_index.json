{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Data Science Hub","text":"<p>Myself Sayan Roy and I am incoming Data Scientist at Celebal Technologies which is a service based company. </p>"},{"location":"#important-pages","title":"Important Pages","text":"<ul> <li><code>Basic Statistics</code> </li> </ul>"},{"location":"distributions%20in%20statistics/","title":"Distributions in Statistics:","text":""},{"location":"distributions%20in%20statistics/#normal-distribution","title":"Normal Distribution","text":"<p>  </p> <p>$f(x) = probability density function$</p> <p>$\\sigma = standard deviation$</p> <p>$\\mu = mean$</p>"},{"location":"distributions%20in%20statistics/#standardization-z-score","title":"Standardization / Z-Score","text":"<p>To compare the data to a standard normal distribution, you subtract the mean and then divided by the standard deviation. This is called standardization or z-score.</p> <p> \\large Z \\ - Score = \\frac{x_i - \\bar{x}}{\\sigma_x} </p>"},{"location":"distributions%20in%20statistics/#qq-plot","title":"QQ-Plot","text":"<p>A QQ - Plot is used to visually determine how close a sample specified distribution - in this case, the normal distribution.</p> <pre><code>import scipy.stats as stats\n\nfig, ax = plt.subplots(figsize=(4, 4))\nnorm_sample = stats.norm.rvs(size=100)\nstats.probplot(norm_sample, plot=ax)\n</code></pre>"},{"location":"distributions%20in%20statistics/#long-tailed-distribution","title":"Long-Tailed Distribution","text":"<p>Sometimes, the distribution is highly skewed, such as with income data or the distribution can be discrete, as with binominal data. The long narrow portion of a frequency distribution, where relatively extreme values occur at low frequency.</p>"},{"location":"measures%20of%20central%20tendency/","title":"Measures of Central Tendency","text":""},{"location":"measures%20of%20central%20tendency/#data-types-in-statistics","title":"Data Types in Statistics","text":"<p>Generally 2 types of data types we can see in Data-Science.</p> <ul> <li><code>Numeric</code>: Data that are expressed on a numeric scale.<ul> <li>Continuous: Data that can take on any value in an interval.</li> <li>Discrete: Data that can take on only integer values, such as counts</li> </ul> </li> <li><code>Categorical</code>: Data that can take on only a specofic set of values representing a set of possible categories.<ul> <li>Binary: A special case of categorical data with just two categories of values e.g., 0/1, true/false.</li> <li>Ordinal: Categorical data that has an explicit ordering.</li> </ul> </li> </ul>"},{"location":"measures%20of%20central%20tendency/#measures-of-central-tendency_1","title":"Measures of Central Tendency","text":""},{"location":"measures%20of%20central%20tendency/#mean","title":"Mean","text":"<p>  </p> <pre><code>DataFrame.mean()\n</code></pre>"},{"location":"measures%20of%20central%20tendency/#trimmed-mean","title":"Trimmed Mean","text":"<p>A variation of the mean is a trimmed mean which you calculate by dropping a fixed number of sorted values at each end and then taking an average of the remaining values.</p> <p>The formula to compute the trimmed mean with $p$ smallest and largest values omitted is: \\large \\bar{x} = \\frac{\\sum_{i = p+1}^{n-p}{x_i}}{n - 2p} </p> <pre><code>import scipy.stats as stats\n\n# 0.1 means drop 10% from each end.\nstats.trim_mean(column_name, 0.1)\n</code></pre>"},{"location":"measures%20of%20central%20tendency/#weighted-mean","title":"Weighted Mean","text":"<p> \\large \\bar{x_w} = \\frac{\\sum_{i=1}^{n}{w_ix_i}}{\\sum_{i=1}^{n}{w_i}} </p> <pre><code>import numpy as np\n\nnp.average(DataFrame.column, weights=DataFrame.another_column)\n</code></pre>"},{"location":"measures%20of%20central%20tendency/#median","title":"Median","text":"<p>Compared to mean, which uses all the observations, the median depends only on the values in the center of the sorted data. It is also possible to compute the weighted median.</p> <pre><code>DataFrame.median()\n</code></pre> <p>For weighted median, you can use the specialized package <code>wquantiles</code>.</p> <pre><code>wquantiles.median(DataFrame.column, weights=DataFrame.another_column)\n</code></pre>"},{"location":"measures%20of%20central%20tendency/#mode","title":"Mode","text":"<p>The mode is the value - or values in case of a tie - that appears most often in the data. The mode is a simple summary statistics for categorical data, and it is generally not used for numeric data.</p>"},{"location":"measures%20of%20central%20tendency/#expected-value","title":"Expected Value","text":"<p>l data is data in which the categories represent or can be mapped to discrete values on the same scale. A marketer for a new cloud technology, for example, offers two levels of service, one priced at $300/month and another at $50/month. The marketer offers free webinars to generate leads, and the firm figures that 5% of the attendees will sign up for the $300 service, 15% will sign up for the $50 service and rest 80% will not sign up for anything. This data can be summed up, for financial purpose, in a single \"expected value,\" which is a form of weighted mean, in which the weights are probabilities. So, expected values are:</p> <p> \\large EV = 0.05 * 300 + 0.15 * 50 + 0.80 * 0 = 22.5 </p> <p>In that cloud service example, the expected value of a webinar attendee is thus $22.50 per month.</p>"},{"location":"measures%20of%20dispersion/","title":"Measures of Dispersion or Variability","text":""},{"location":"measures%20of%20dispersion/#mean-absolute-deviation-mad","title":"Mean Absolute Deviation (MAD)","text":"<p>  </p>"},{"location":"measures%20of%20dispersion/#variance-standard-deviation","title":"Variance &amp; Standard Deviation","text":"<p> \\large Variance \\ (s^2) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n-1} </p> <p> \\large STD \\ (s) = \\sqrt{s^2} = \\sqrt{\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}} </p> <pre><code># STD calculation\nDataFrame.std()\n</code></pre>"},{"location":"measures%20of%20dispersion/#median-absolute-variance","title":"Median Absolute Variance","text":"<p> \\large Median \\ Absolute \\ Variance = Median(|x_1 - m|, |x_2 - m|, ..., |x_n - m|) </p> <p>where $m$ is the median.</p>"},{"location":"measures%20of%20dispersion/#range","title":"Range","text":"<p>The difference between the largest and the smallest numbers. The minimum and maximum values themselves are useful to know and are helpful in identifying outliers. But the range is extremely sensitive to outliers and not very useful as a general measure of dispersion in the data.</p>"},{"location":"measures%20of%20dispersion/#percentile-iqr","title":"Percentile | IQR","text":"<p>In a dataset, the $P$ th percentile is a value such that at least $P$ percent of the values take on this value or less and at least $(100-P)$ percent of the values take on this value or more.</p> <p>The $median$ is the same thing as the 50th percentile. A common measurement of variability is the difference between the 25th percentile and the 75th percentile, called Interquartile Range (IQR). For very large datasets, calculating exact percentiles can be computationally very expensive since it requires sorting all the data values.</p> <pre><code># calculate IQR\nDataFrame[column_name].quantile(0.75) - DataFrame[column_name].quantile(0.25)\n</code></pre>"},{"location":"measures%20of%20dispersion/#correlation","title":"Correlation","text":"<p>Variables $X$ and $Y$ (each with measured data) are said to be <code>positively correlated</code> if high values of $X$ go with high values of $Y and low values of $X$ go to with low values of $Y. If high values of $X go with low values of $Y, and vice versa, the variables are <code>negatively correlated</code>.</p> <ul> <li><code>Correlation coefficient</code>: A metric that measures the extent to which numeric variables are associated with one another (ranges from -1 to +1)</li> <li><code>Correlation matrix</code>: A table where the variables are shown on both rows and columns, and the cell values are the correlations between the variables.</li> </ul> <p> Pearson's \\ correlation \\ coefficient \\ (r) = \\frac{\\sum_{i=1 }^{n}{(x_i - \\bar{x})(y_i - \\bar{y})}}{(n-1)s_xs_y} </p> <pre><code>import seaborn as sns\n\nsns.heatmap(DataFrame.corr(), vmin=-1, vmax=1, cmap=sns.diverging_palette(20, 220, as_cmap=True))\n</code></pre> <p>Like the <code>mean</code> and <code>standard deviation</code>, the correlation coefficient is sensitive to outliers in the data. <code>Spearman's rho</code> and <code>Kendall's tau</code> correlation techniques are based on the rank of the data and these are robust to outliers and can handle certain types of nonlinearities.</p>"},{"location":"measures%20of%20dispersion/#pivot-table","title":"Pivot Table","text":"<p>Whether the correlation coefficient helps to measure between two numeric columns, pivot table helps to measure between two categorical columns.</p> <pre><code>crosstab = DataFrame.pivot_table(index=category_col_1, columns=category_col_2, aggfunc=lambda x: len(x), margins=True)\ncrosstab\n</code></pre>"},{"location":"sampling/","title":"Data Sampling","text":""},{"location":"sampling/#random-sampling","title":"Random Sampling","text":"<p>Random Sampling is the process in which each available member of the population being sampled has an equal chance of being chosen for the sample at each draw. The sample that results is called a <code>simple random sample</code>. Sampling can be down <code>with replacement</code>, in which observations are put back in the population after each draw for possible future reselection. Or it can be done <code>without replacement</code>, in which case observations once selected are unavailable for future draws.</p> <p>In <code>stratified sampling</code>, the population is divided up into <code>strata</code>, and random samples are taken from each stratum.</p>"},{"location":"sampling/#statistical-bias","title":"Statistical Bias","text":"<p>Statistical bias refers to measurement or sampling errors that are <code>systematic</code> and <code>produced by the measurement</code> or <code>sampling process</code>. </p> <p>Consider the physical process of a gun shooting at a target. It will not hit the absolute center of the target every time or even much at all. An unbiased process will produce error, but it is random and does not tend strongly in any direction. </p> <p>Bias comes in different forms, may be <code>observable</code> or <code>invisible</code>. When a result does suggest bias, it is often an indicator that a statistical or machine learning model has been misspecified or an important variable left out.</p>"},{"location":"sampling/#selection-bias","title":"Selection Bias","text":"<p>Selection bias refers to the practice of selectively choosing data - consciously or unconsciously - in a way that leads to a conclusion that is misleading or ephemeral.</p> <p><code>Data snooping</code> is extensive hunting through the data until something interesting emerges. There is a saying among statisticians: \"If you torture the data long enough, sooner or later it will confess.\"</p> <p>If you repeatedly run different models and ask different questions with a large data set, you are bound to find something interesting. This is called <code>vast search effect.</code></p>"},{"location":"sampling/#central-limit-theorem-sampling-distribution","title":"Central Limit Theorem / Sampling Distribution","text":"<p>The statistics (e.g., mean) drawn from multiple samples will resemble the familiar bell-shaped normal curve, even if the source population is not normally distributed, provided that the sample size is large enough and the departure of the data from normality is not too great.</p>"},{"location":"sampling/#standard-error","title":"Standard Error","text":"<p>The standard error is a single metric that sums up the variability in the sampling distribution for a statistics.</p> <p>  </p> <p>Consider the following approach to measuring standard error: 1. Collect a number of brand-new samples from the population. 2. For each new sample, calculate the statistics (e.g., mean) 3. Calculate the standard deviation of the statistics computed in step 2; use this as your estimate of standard error.</p>"},{"location":"sampling/#bootstrapping","title":"Bootstrapping","text":"<p>One easy and effective way to estimate the sampling distribution of a statistics, or of model parameters is to draw additional <code>samples, with replacement</code> from the sample itself and recalculate the statistics or model for each resample. This procedure is called <code>bootstrap</code>. It does not necessarily involve any assumptions about the data or the sample statistics being normally distributed. </p> <p>The algorithm for a bootstrap resampling of the mean, for a sample of size $n$, is as follows: 1. Draw a sample value, record it and then replace it. 2. Repeat $n$ times. 3. Record the mean of the $n$ resampled values. 4. Repeat steps 1-3 $R$ times. 5. Use the $R$ results to:     1. Calculate their standard deviation (this estimates sample mean standard error).     2. Procedure a histogram or boxplot.     3. Find a confidence interval.</p> <pre><code>from sklearn.utils import resample\n\nresults = []\nfor nrepeat in range(1000):\n    sample = resample(DataFrame[column_name])\n    results.append(sample.mean())\n\nresults = pd.Series(results)\nprint('Bootstrap Statistics:')\nprint(f'original: {DataFrame[column_name].median()}')\nprint(f\"bias: {results.median() - DataFrame[column_name].median()}\")\nprint(f\"std. error: {results.std()}\")\n</code></pre>"},{"location":"sampling/#confidence-interval-ci","title":"Confidence Interval (CI)","text":"<p>Confidence intervals always come with a coverage level, expressed as a (high) percentage, say 90% or 95%. One way to think of a 90% confidence interval is as follows: it is the interval that encloses the central 90% of the bootstrap sampling distribution of a sample statistic. More generally, an $x\\%$ confidence interval around a sample estimate should, on average, contain similar sample estimates $x\\%$ of the time.</p> <p>Given a sample of size $n$, and a sample statistic of interest, the algorithm for a bootstrap confidence interval is as follows: 1. Draw a random sample of size $n$ with replacement from the data (a resample). 2. Record the statistics of interest for the resample. 3. Repeat steps 1-2 many ($R$) times. 4. For an $x\\%$ confidence interval, trim $[(100-x)/2]\\%$ of the $R$ resample results from either end of the distribution. 5. The trim points are the endpoints of an $x\\%$ bootstrap confidence interval.</p> <p>The higher the level of confidence, the wider the interval. Also, the smaller the sample, the wider the interval (i.e., the greater the uncertainty).</p>"},{"location":"visualization/","title":"Visualization","text":""},{"location":"visualization/#boxplot","title":"Boxplot","text":"<pre><code>DataFrame[column_name].plot.box()\n</code></pre>  <p>Boxplots are a simple way to visually compare the distributions of a numeric variable grouped accordingly to a categorical variable. The pandas boxplot method takes the <code>by</code> argument that splits the data set into groups and creates the individual boxplots:</p> <pre><code>ax = DataFrame.boxplot(by=category_col_1, column=numeric_col_1)\nax.set_xlabel(category_col_1)\nax.set_ylabel(numeric_col_1)\n</code></pre>"},{"location":"visualization/#histogram","title":"Histogram","text":"<pre><code>ax = DataFrame[column_name].plot.hist(figsize=(4,4))\nax.set_xlabel(\"Some label based on X-Axis)\n</code></pre>"},{"location":"visualization/#densitykde-plot","title":"Density/KDE Plot","text":"<pre><code>ax = DataFrame[column_name].plot.hist(density=True)\nDataFrame[column_name].plot.density(ax=ax)\nax.set_xlabel(\"Set some label of X-axis)\n</code></pre>"},{"location":"visualization/#bar-chart","title":"Bar chart","text":"<p>Note that a bar chart resembles a histogram; in a bar chart the x-axis represents different categories of a factor variable, while in a histogram the x-axis represents values of a single variable on a numeric scale. In a histogram, the bars are typically shown touching each other, with gaps indicating values that did not occur in the data. In a bar chart, the bars are shown separate from one another.</p> <pre><code>ax = DataFrame[column_name].plot.bar(figsize = (4, 4), legend=False)\nax.set_xlable(\"Set some x-axis label\")\nax.set_ylabel(\"Count\")\n</code></pre>"},{"location":"visualization/#scatter-plots","title":"Scatter plots","text":"<p>The standard way to visualize the relationship between two measured data variables is with a scatter plot. The x-axis represents one variables and the y-axis represents another, each point on the graph is a record.</p> <p>Scatter plots are fine when there is a relatively small number of data values. For data sets with hundred of thousands or millions of records, a scatter plot will be too dense to identify the details of the data. </p> <pre><code>ax = DataFrame.plot.scatter(x=column_name_1, y=column_name_2, figsize=(4, 4), marker='$\\u25EF$')\nax.set_xlabel(column_name_1)\nax.set_ylabel(column_name_2)\nax.axhline(0, color=\"grey\", lw=1)\nax.axvline(0, color=\"grey\", lw=1)\n</code></pre>"},{"location":"visualization/#hexagonal-binning-plot","title":"Hexagonal Binning Plot","text":"<p>This plot is the solution of the disadvantages of scatter plot. Rather than plotting points, which would appear as a monolithic dark cloud, we can group the records into hexagonal bins and then can plot that hexagons with a color indicating the number of records in that bin.</p> <pre><code>ax = DataFrame.plot.hexbin(x=column_name_1, y=column_name_2, gridsize=30, sharex=False, figsize=(5, 4))\nax.set_xlabel(column_name_1)\nax.set_ylabel(column_name_2)\n</code></pre>"},{"location":"visualization/#contour-plot","title":"Contour Plot","text":"<p>Another solution of the disadvantages of scatter plot. The contour plot overlaid onto a scatter plot to visualize the relationship between two numeric variables. The contours are essentially a topographical map to two variables; each contour band represents a specific density of points, increasing as one nears a \"peak\".</p> <pre><code>import seaborn as sns\n\nax = sns.kdeplot(DataFrame[column_name_1], DataFrame[column_name_2], ax=ax)\nax.set_xlabel(column_name_1)\nax.set_ylabel(column_name_2)\n</code></pre>"},{"location":"visualization/#heat-maps","title":"Heat Maps","text":"<p>Another solution of the disadvantages of the scatter plot.</p>"},{"location":"visualization/#violin-plot","title":"Violin Plot","text":"<p>A violin plot is an enhancement to the boxplot and plots the density estimate with the density on the y-axis. The density is mirrored and flipped over, and the resulting shape is filled in, creating an image resembling a violin. The advantage of a violin plot is that it can show nuances in the distribution that aren't perceptible in a boxplot. On the other hand, the boxplot more clearly shows the outliers in the data.</p> <pre><code>import seaborn as sns\n\nax = sns.violineplot(DataFrame[categorical_col_1], DataFrame[numerical_col_1], inner=\"quartile\", color=\"white\")\nax.set_xlabel(categorical_col_1)\nax.set_ylabel(numerical_col_1)\n</code></pre>"}]}